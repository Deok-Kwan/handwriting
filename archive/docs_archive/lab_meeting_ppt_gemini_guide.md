# MIL 프로젝트 랩미팅 PPT - Gemini 작성 가이드

## 슬라이드별 구성 및 스크립트

### 슬라이드 1: 제목
**내용:**
- 제목: 문서 저자 유형 판별을 위한 MIL 접근법 연구: Siamese Network 적용 및 한계점 분석
- 부제: Multiple Instance Learning 기반 문서 저자 유형 판별 (단일 vs 복수 저자)
- 발표자: [이름]
- 소속: [연구실]
- 날짜: 2025년 6월 XX일

**스크립트 (1분):**
"안녕하십니까. 오늘 랩미팅에서는 'Multiple Instance Learning 기반 문서 저자 유형 판별' 프로젝트의 진행 상황에 대해 발표하겠습니다. 지난번 Autoencoder 모델의 한계를 보고드린 이후, 새로운 접근법으로 Siamese Network를 도입하여 실험을 진행했습니다. 오늘은 그 실험 결과와 과정에서 발견한 문제점, 그리고 향후 계획에 대해 공유드리고자 합니다."

---

### 슬라이드 2: 지난 연구 요약
**내용:**
- 목표: 문서 내 텍스트 조각들의 스타일 유사도를 학습하여 단일/복수 저자를 판별
- 이전 접근법: Autoencoder 기반 재구성 오류 활용
- 결과: 정확도 50.2% (무작위 추측 수준)
- 결론: 재구성 오류만으로는 저자 스타일의 미묘한 차이를 포착하기에 불충분

**스크립트 (1분):**
"먼저 지난 연구를 간략히 요약하겠습니다. 저희의 목표는 문서 내 여러 텍스트 조각들의 스타일을 분석해, 문서 전체가 단일 저자에 의해 쓰였는지, 혹은 여러 저자에 의해 쓰였는지를 판별하는 것입니다. 이를 위해 Autoencoder를 사용해 단일 저자 문서의 스타일을 학습시키고, 복수 저자 문서에서 나타나는 이질적인 스타일을 '재구성 오류'가 높게 나타나는 것으로 탐지하고자 했습니다. 하지만 보시는 바와 같이 정확도는 50.2%에 그쳤습니다. 이는 모델이 저자 스타일의 복잡하고 미묘한 차이를 전혀 학습하지 못했다는 것을 의미하며, 새로운 접근법의 필요성을 확인했습니다."

---

### 슬라이드 3: Siamese Network 소개
**내용:**
- 개념: 두 개의 동일한 가중치를 공유하는 네트워크를 사용하여 입력 쌍의 유사도를 측정
- 작동 원리:
  1. 두 입력을 동일한 인코더에 통과시켜 임베딩 벡터 추출
  2. 두 임베딩 벡터 간의 거리 계산
  3. Contrastive Loss로 같은 클래스는 가깝게, 다른 클래스는 멀게 학습
- 우리 문제에의 적용: "두 텍스트 조각이 같은 저자 그룹에서 나왔는가?"를 학습

**스크립트 (1분 30초):**
"Autoencoder의 한계를 극복하기 위해, 저희는 '유사도 학습'에 특화된 Siamese Network를 도입했습니다. Siamese Network는 보시는 그림과 같이, 완전히 동일한 두 개의 네트워크를 사용해 입력으로 들어온 한 쌍의 데이터가 얼마나 유사한지를 직접적으로 학습하는 구조입니다. 두 텍스트 조각을 각각 동일한 인코더에 넣어 임베딩 벡터로 변환한 뒤, 이 두 벡터 사이의 거리를 측정합니다. 그리고 Contrastive Loss 함수를 통해, 같은 저자 그룹에서 나온 텍스트 조각 쌍은 임베딩 공간에서 가깝게 만들고, 다른 저자 그룹에서 나온 쌍은 멀어지도록 네트워크를 훈련시킵니다."

---

### 슬라이드 4: 실험 설계
**내용:**
- 데이터 구성:
  - Positive Pair: 같은 복수 저자 문서에서 추출한 두 개의 다른 텍스트 조각
  - Negative Pair: 복수 저자 문서의 텍스트 조각 1개 + 단일 저자 문서의 텍스트 조각 1개
- 모델 구조:
  - Backbone Encoder: Pre-trained ViT (300차원)
  - Distance Metric: Euclidean Distance
  - Loss Function: Contrastive Loss
- 평가 방식: 테스트 문서에서 텍스트 조각 쌍들의 평균 유사도로 판별

**스크립트 (1분 30초):**
"실험 설계에 대해 말씀드리겠습니다. 모델 학습을 위해 입력 쌍을 구성하는 것이 중요한데, 'Positive Pair'는 하나의 복수 저자 문서에서 가져온 서로 다른 두 텍스트 조각으로 정의했습니다. 같은 문서에서 나왔으니 스타일적으로 유사할 것이라는 가정입니다. 'Negative Pair'는 복수 저자 문서의 조각 하나와, 단일 저자 문서의 조각 하나를 짝지었습니다. 모델의 인코더로는 사전학습된 Vision Transformer를 사용했고, Contrastive Loss를 통해 학습을 진행했습니다."

---

### 슬라이드 5: 첫 번째 Siamese 실험 결과
**내용:**
- 결과: 정확도 64.4% 달성
- 분석:
  - Autoencoder(50.2%) 대비 14.2%p 성능 향상
  - Siamese Network가 저자 스타일의 유사성을 판별하는 데 유의미한 특징을 학습
  - 아직 개선의 여지가 많은 성능
- 시사점: 접근 방향이 올바르다는 긍정적인 신호

**스크립트 (1분):**
"첫 번째 Siamese Network 실험 결과입니다. 보시는 바와 같이, 정확도 64.4%를 달성했습니다. 이는 기존 Autoencoder 방식의 50.2%와 비교했을 때 약 14.2%p 향상된 수치입니다. 이 결과는 Siamese Network가 단순히 텍스트를 재구성하는 것보다 저자 스타일의 유사성을 비교하는 데 훨씬 효과적이며, 저희가 설정한 접근 방향이 올바르다는 강력한 증거라고 생각합니다."

---

### 슬라이드 6: 개선 전략
**내용:**
- 가설: 모델이 Positive와 Negative Pair를 더 명확하게 구분하도록 학습을 강화하면 성능 향상
- 전략:
  1. Margin 값 조정: Contrastive Loss의 margin 값을 증가 (1.0 → 2.0)
  2. 학습률 조정: 더 세밀한 학습을 위해 학습률을 낮춤
  3. 데이터 증강: 훈련 데이터의 다양성 확보
- 기대 효과: 임베딩 공간에서 클래스 간 경계가 더 명확해져 판별 능력 향상

**스크립트 (1분 30초):**
"64.4%의 성능을 더 끌어올리기 위해, 모델이 특징을 더 잘 학습하도록 제약을 강화하는 전략을 세웠습니다. 저희의 가설은, 임베딩 공간에서 Positive Pair와 Negative Pair를 더 확실하게 분리하면 성능이 오를 것이라는 점이었습니다. 이를 위해 Contrastive Loss의 핵심 파라미터인 'Margin' 값을 기존 1.0에서 2.0으로 높였습니다. 이와 함께 학습률을 미세 조정하고 데이터 증강을 통해 모델의 일반화 성능을 높이고자 했습니다."

---

### 슬라이드 7: 개선된 Siamese 실험 결과
**내용:**
- 결과: 정확도 56.5%로 오히려 하락
- 현상:
  - 실험 1 (64.4%) 대비 7.9%p 성능 저하
  - 예측 결과가 특정 클래스로 편중
  - Loss는 매우 낮은 값으로 수렴했지만, 실제 성능은 나빠짐
- 결론: 의도했던 개선 전략이 예상치 못한 부작용을 야기함

**스크립트 (1분):**
"하지만 개선 전략을 적용한 두 번째 실험 결과는 예상과 완전히 달랐습니다. 정확도가 64.4%에서 오히려 56.5%로 크게 하락했습니다. 더 이상한 점은, 학습 과정에서 Loss는 매우 안정적으로 낮은 값에 수렴했는데도 불구하고 실제 테스트 정확도는 더 나빠졌다는 것입니다. 이는 모델이 Loss를 낮추는 쉬운 길, 즉 '꼼수'를 찾았을 가능성을 시사합니다."

---

### 슬라이드 8: 임베딩 붕괴 분석
**내용:**
- 핵심 원인: 임베딩 붕괴(Embedding Collapse) 현상 발생
- 정의: 모델이 모든 입력을 임베딩 공간의 거의 동일한 한 점으로 매핑
  - 모든 쌍의 거리가 0에 가까워짐
  - Positive Pair의 Loss는 0, Negative Pair의 Loss는 margin 값으로 수렴
  - 모델은 더 이상 입력의 차이를 구분하려 노력하지 않음
- t-SNE 시각화: 모든 데이터 포인트가 하나의 점으로 뭉쳐버림

**스크립트 (1분 30초):**
"원인 분석 결과, 저희는 '임베딩 붕괴'라는 현상을 발견했습니다. 임베딩 붕괴란, 모델이 Loss를 가장 쉽게 낮추기 위해, 모든 입력 데이터를 임베딩 공간 상의 그냥 한 점으로 뭉쳐버리는 현상입니다. 이렇게 되면 모든 쌍의 거리가 0에 가까워지므로, Positive Pair에 대한 Loss는 0이 되고, Negative Pair에 대한 Loss는 margin 값으로 일정하게 유지됩니다. 오른쪽 t-SNE 시각화 자료가 이를 명확히 보여줍니다."

---

### 슬라이드 9: 해결 방안
**내용:**
- Margin 값 재조정 및 Grid Search
  - 0.5 ~ 2.0 사이에서 최적의 margin 값 탐색
- Triplet Loss 도입 검토
  - (Anchor, Positive, Negative) 세 쌍을 이용한 더 강건한 학습
- Hard Negative/Positive Mining
  - 경계선에 가까운 어려운 데이터들을 집중 학습
- 데이터 불균형 문제 재검토
  - Positive/Negative Pair의 비율 조정

**스크립트 (2분):**
"임베딩 붕괴 문제를 해결하기 위해 다음과 같은 방안들을 계획하고 있습니다. 첫째, 붕괴의 직접적 원인으로 추정되는 margin 값을 체계적인 Grid Search를 통해 최적의 값을 찾겠습니다. 둘째, Contrastive Loss보다 붕괴 현상에 더 강건하다고 알려진 Triplet Loss 도입을 적극 검토하겠습니다. Triplet Loss는 기준점인 Anchor를 사용해, Positive 샘플은 가깝게, Negative 샘플은 'Positive보다 확실히 더 멀리' 떨어뜨리도록 학습합니다. 셋째, 모델이 쉽게 맞추는 데이터 대신, 판별하기 어려운 'Hard' 샘플들을 집중적으로 학습시키는 Hard Mining 기법을 적용할 계획입니다."

---

### 슬라이드 10: 전체 연구 로드맵
**내용:**
- Phase 1: 초기 접근 (완료)
  - Autoencoder 기반 모델링 및 한계 확인
- Phase 2: Siamese Network 도입 (진행 중)
  - 기본 모델 구현 및 가능성 확인
  - **임베딩 붕괴 문제 분석 및 해결 (현재 위치)**
  - Triplet Loss 적용 및 하이퍼파라미터 튜닝
- Phase 3: 모델 고도화 및 최종 평가
  - Hard Mining 적용 및 성능 최적화
  - 최종 모델 확정 및 결과 정리
- Phase 4: 논문 작성

**스크립트 (1분):**
"저희 연구의 전체적인 로드맵입니다. Phase 1에서 Autoencoder의 한계를 확인했고, 현재는 Phase 2, Siamese Network를 도입하는 단계에 있습니다. 구체적으로는 오늘 보고드린 '임베딩 붕괴 문제 분석 및 해결' 단계에 위치해 있습니다. 앞으로는 Triplet Loss 적용과 하이퍼파라미터 튜닝을 통해 이 문제를 해결하고, Phase 3에서 모델을 최종적으로 고도화할 계획입니다."

---

### 슬라이드 11: 핵심 통찰 및 교훈
**내용:**
- Siamese Network의 가능성: 저자 스타일 유사도 측정에서 Autoencoder보다 유망함
- '개선'의 함정: 의도된 개선이 예기치 않은 부작용을 낳을 수 있음
- Loss와 Accuracy의 괴리: 학습 Loss가 낮아지는 것이 실제 성능 향상을 보장하지 않음
- 실패는 데이터다: 실험 2의 실패로 '임베딩 붕괴'라는 중요한 현상을 이해

**스크립트 (1분 30초):**
"이번 실험 사이클을 통해 몇 가지 중요한 통찰과 교훈을 얻었습니다. 첫째, Siamese Network는 저희 문제에 매우 적합하고 유망한 접근법이라는 확신을 얻었습니다. 둘째, 단순히 하이퍼파라미터를 강하게 조절하는 것이 항상 좋은 결과를 낳는 것은 아니며, '임베딩 붕괴'와 같은 예상치 못한 함정이 있다는 것을 배웠습니다. 특히 Loss 값만 보고 모델이 잘 학습되고 있다고 착각해서는 안 되며, 모델의 내부 작동 방식을 반드시 함께 들여다봐야 한다는 점을 깨달았습니다."

---

### 슬라이드 12: 결론
**내용:**
- 현황: Siamese Network가 64.4%의 초기 성능으로 가능성을 보였으나, 개선 시도 중 임베딩 붕괴로 56.5%로 하락
- 원인: 과도한 margin 설정으로 인한 학습 불안정성
- 향후 계획:
  - 단기: Triplet Loss 도입 및 하이퍼파라미터 최적화
  - 장기: 모델 고도화 및 최종 평가
- 기대: 체계적인 문제 해결을 통해 안정적이고 높은 성능의 모델 개발

**스크립트 (1분):**
"결론입니다. 저희는 저자 유형 판별 문제를 풀기 위해 Siamese Network를 성공적으로 도입하여 초기 가능성을 확인했습니다. 하지만 모델을 개선하는 과정에서 임베딩 붕괴라는 기술적 난관에 부딪혔습니다. 저희는 이 원인을 분석했으며, 이를 해결하기 위한 명확한 다음 계획을 수립했습니다. 체계적인 접근을 통해 이 문제를 충분히 해결할 수 있을 것이라 생각합니다."

---

### 슬라이드 13: Q&A
**내용:**
- 감사합니다
- 질문 있으신가요?

**스크립트:**
"경청해주셔서 감사합니다. 질문 있으시면 편하게 해주시기 바랍니다."