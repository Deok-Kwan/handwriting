{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c918235",
   "metadata": {},
   "source": [
    "# Stage 3 TransformerMIL: Transformer-based MIL Model\n",
    "\n",
    "이 노트북은 Stage 2에서 생성한 MIL Bag 데이터를 입력으로 받아 Transformer 기반 MIL 모델을 학습하고 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39044d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경 설정\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, roc_curve, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "# GPU 설정\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = os.getenv('MIL_STAGE3_GPU', '3')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "if device.type == 'cuda':\n",
    "    print('GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('CUDA를 사용할 수 없습니다. CPU 모드로 실행됩니다.')\n",
    "\n",
    "# 시드 고정\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4742eb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2 Bag 데이터 로드 및 Instance 평균 계산\n",
    "embedding_margin = '0.4'\n",
    "bags_dir = '/workspace/MIL/data/processed/bags'\n",
    "train_pkl = os.path.join(bags_dir, f'bags_arcface_margin_{embedding_margin}_50p_random_train.pkl')\n",
    "val_pkl   = os.path.join(bags_dir, f'bags_arcface_margin_{embedding_margin}_50p_random_val.pkl')\n",
    "test_pkl  = os.path.join(bags_dir, f'bags_arcface_margin_{embedding_margin}_50p_random_test.pkl')\n",
    "\n",
    "print('Loading MIL bags...')\n",
    "with open(train_pkl, 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "with open(val_pkl, 'rb') as f:\n",
    "    val_data = pickle.load(f)\n",
    "with open(test_pkl, 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "# Instance mean 계산: (10,5,256) → (10,256)\n",
    "def to_instance_means(bags):\n",
    "    return [bag.mean(axis=1).astype(np.float32) for bag in bags]\n",
    "\n",
    "train_features = to_instance_means(train_data['bags'])\n",
    "val_features   = to_instance_means(val_data['bags'])\n",
    "test_features  = to_instance_means(test_data['bags'])\n",
    "\n",
    "train_labels = train_data['labels']\n",
    "val_labels   = val_data['labels']\n",
    "test_labels  = test_data['labels']\n",
    "\n",
    "print(f'Train bags: {len(train_labels)}, Val bags: {len(val_labels)}, Test bags: {len(test_labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dbf94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 클래스 (on‑the‑fly Tensor 변환)\n",
    "\n",
    "class MILDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features  # list of np.ndarray\n",
    "        self.labels = labels      # list of int\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return x, y\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(MILDataset(train_features, train_labels), batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(MILDataset(val_features,   val_labels),   batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(MILDataset(test_features,  test_labels),  batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79407013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Models: PosEnc & TransformerMIL\n",
    "# =========================\n",
    "import math\n",
    "\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 512, dropout_p: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        pe = torch.zeros(max_len, d_model)  # [max_len, d_model]\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2]  = torch.sin(position * div_term)\n",
    "        pe[:, 1::2]  = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)  # not a parameter\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: [B, N, d_model]\n",
    "        \"\"\"\n",
    "        B, N, D = x.size()\n",
    "        x = x + self.pe[:N].unsqueeze(0)  # [1, N, D] broadcast\n",
    "        return self.dropout(x)\n",
    "\n",
    "class AttentionPooler(nn.Module):\n",
    "    \"\"\"\n",
    "    ABMIL 스타일의 소프트 어텐션 풀러.\n",
    "    입력 [B, N, d_model] -> 가중합 [B, d_model] + weights [B, N]\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, hidden: int = 128, dropout_p: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, 1)\n",
    "        self.drop = nn.Dropout(dropout_p)\n",
    "        nn.init.xavier_uniform_(self.fc1.weight); nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight); nn.init.zeros_(self.fc2.bias)\n",
    "\n",
    "    def forward(self, H):  # H: [B, N, d_model]\n",
    "        A = torch.tanh(self.fc1(self.drop(H)))   # [B, N, hidden]\n",
    "        A = self.fc2(A).squeeze(-1)              # [B, N]\n",
    "        weights = torch.softmax(A, dim=1)        # [B, N]\n",
    "        Z = torch.sum(weights.unsqueeze(-1) * H, dim=1)  # [B, d_model]\n",
    "        return Z, weights\n",
    "\n",
    "class TransformerMIL(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer 기반 MIL (범주) - 인스턴스 간 self-attention + attention pooling 집계.\n",
    "    forward: (logits, weights) 반환 -> baseline 학습 루프와 호환.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=256,      # ArcFace 임베딩 차원\n",
    "        d_model=128,\n",
    "        nhead=4,\n",
    "        num_layers=2,\n",
    "        dim_feedforward=256,\n",
    "        dropout_p=0.1,\n",
    "        pos_enc='sin'      # 'sin' or 'learned'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(input_dim, d_model)\n",
    "        if pos_enc == 'sin':\n",
    "            self.posenc = SinusoidalPositionalEncoding(d_model, max_len=128, dropout_p=dropout_p)\n",
    "        else:\n",
    "            self.pos_embedding = nn.Embedding(128, d_model)  # N<=128 가정\n",
    "            nn.init.normal_(self.pos_embedding.weight, std=0.02)\n",
    "            self.posenc = None\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout_p, batch_first=True, norm_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "        self.pooler  = AttentionPooler(d_model, hidden=128, dropout_p=dropout_p)\n",
    "        self.classifier = nn.Linear(d_model, 1)\n",
    "\n",
    "        # init\n",
    "        nn.init.xavier_uniform_(self.proj.weight); nn.init.zeros_(self.proj.bias)\n",
    "        nn.init.xavier_uniform_(self.classifier.weight); nn.init.zeros_(self.classifier.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [B, N, D]\n",
    "        returns: logits[B], weights[B,N]\n",
    "        \"\"\"\n",
    "        B, N, D = x.shape\n",
    "        h = self.proj(x)  # [B, N, d_model]\n",
    "        if self.posenc is not None:\n",
    "            h = self.posenc(h)     # sinusoidal\n",
    "        else:\n",
    "            idx = torch.arange(N, device=h.device).unsqueeze(0).repeat(B,1)\n",
    "            h = h + self.pos_embedding(idx)\n",
    "\n",
    "        h = self.encoder(h)        # [B, N, d_model]\n",
    "        z_bag, weights = self.pooler(h)   # [B, d_model], [B, N]\n",
    "        logits = self.classifier(z_bag).squeeze(-1)  # [B]\n",
    "        return logits, weights\n",
    "\n",
    "# 모델 인스턴스 (AttentionMIL 대신 TransformerMIL 사용)\n",
    "mil_model_final = TransformerMIL(\n",
    "    input_dim=256, d_model=128, nhead=4, num_layers=2,\n",
    "    dim_feedforward=256, dropout_p=0.1, pos_enc='sin'\n",
    ").to(device)\n",
    "\n",
    "# 손실 함수 및 최적화 기법\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer_final = torch.optim.Adam(mil_model_final.parameters(), lr=1e-3)\n",
    "scheduler_final = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_final, mode='max', factor=0.5, patience=1, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bb9c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 및 평가 함수 (Early Stopping)\n",
    "\n",
    "def train_one_epoch(model, optimizer, loader):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    preds_all = []\n",
    "    labels_all = []\n",
    "    for X, y in tqdm(loader, desc='Train', leave=False):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X)[0] if isinstance(model, TransformerMIL) else model(X)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * y.size(0)\n",
    "        preds = (torch.sigmoid(logits) >= 0.5).float()\n",
    "        preds_all.extend(preds.cpu().numpy())\n",
    "        labels_all.extend(y.cpu().numpy())\n",
    "    return total_loss / len(loader.dataset), accuracy_score(labels_all, preds_all)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    probs_all = []\n",
    "    preds_all = []\n",
    "    labels_all = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in tqdm(loader, desc='Eval', leave=False):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            logits = model(X)[0] if isinstance(model, TransformerMIL) else model(X)\n",
    "            loss = criterion(logits, y)\n",
    "            total_loss += loss.item() * y.size(0)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs >= 0.5).float()\n",
    "            probs_all.extend(probs.cpu().numpy())\n",
    "            preds_all.extend(preds.cpu().numpy())\n",
    "            labels_all.extend(y.cpu().numpy())\n",
    "    acc = accuracy_score(labels_all, preds_all)\n",
    "    auc = roc_auc_score(labels_all, probs_all) if len(set(labels_all)) > 1 else 0.0\n",
    "    f1 = f1_score(labels_all, preds_all) if len(set(preds_all)) > 1 else 0.0\n",
    "    return total_loss / len(loader.dataset), acc, auc, f1, np.array(probs_all), np.array(labels_all), np.array(preds_all)\n",
    "\n",
    "def train_model(model, optimizer, scheduler, train_loader, val_loader, max_epochs=10, patience=3, name='model'):\n",
    "    best_auc = 0.0\n",
    "    best_state = None\n",
    "    epochs_no_improve = 0\n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        print(f\"\\nEpoch {epoch}/{max_epochs} – {name}\")\n",
    "        tr_loss, tr_acc = train_one_epoch(model, optimizer, train_loader)\n",
    "        val_loss, val_acc, val_auc, val_f1, _, _, _ = evaluate(model, val_loader)\n",
    "        print(f\"  Train Loss: {tr_loss:.4f}, Acc: {tr_acc:.4f}\")\n",
    "        print(f\"  Val   Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, AUC: {val_auc:.4f}, F1: {val_f1:.4f}\")\n",
    "        scheduler.step(val_auc)\n",
    "        if val_auc > best_auc:\n",
    "            best_auc = val_auc\n",
    "            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "            torch.save(best_state, f'best_{name}.pth')\n",
    "            print(f\"  ✅ New best AUC: {best_auc:.4f} – model saved.\")\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"  No improvement. Patience: {epochs_no_improve}/{patience}\")\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"  🛑 Early stopping triggered.\")\n",
    "                break\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071c244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Final Pipeline: Weighted BCE training with validation‑based threshold search\n",
    "#\n",
    "# 이 셀은 가중 BCE(음성 클래스 FP 가중치 2.0)를 이용해 TransformerMIL 모델을 학습하고,\n",
    "# validation 세트에서 F1 기준으로 최적 threshold를 찾아 테스트 성능을 산출합니다.\n",
    "# 또한 Confusion Matrix와 ROC 곡선 시각화를 제공합니다.\n",
    "\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_curve, auc, confusion_matrix\n",
    "\n",
    "# Define Weighted BCE loss\n",
    "class WeightedBCE(nn.Module):\n",
    "    def __init__(self, fp_weight=2.0):\n",
    "        super().__init__()\n",
    "        self.fp_weight = fp_weight\n",
    "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
    "    def forward(self, logits, labels):\n",
    "        loss = self.bce(logits, labels)\n",
    "        fp_mask = (labels == 0).float()\n",
    "        loss = loss * (1 + self.fp_weight * fp_mask)\n",
    "        return loss.mean()\n",
    "\n",
    "# Use Weighted BCE as the criterion for the final model\n",
    "criterion = WeightedBCE(fp_weight=2.0)\n",
    "\n",
    "# Train the model\n",
    "mil_model_final = train_model(\n",
    "    mil_model_final, optimizer_final, scheduler_final,\n",
    "    train_loader, val_loader, max_epochs=10, patience=3, name='transformer_mil'\n",
    ")\n",
    "\n",
    "# Evaluate on validation and test\n",
    "val_loss_final, val_acc_final, val_auc_final, val_f1_final, val_probs_final, val_labels_final, _ = evaluate(\n",
    "    mil_model_final, val_loader\n",
    ")\n",
    "test_loss_final, test_acc_final, test_auc_final, test_f1_final, test_probs_final, test_labels_final, _ = evaluate(\n",
    "    mil_model_final, test_loader\n",
    ")\n",
    "\n",
    "# Function to find best threshold based on F1\n",
    "def find_best_threshold(probs, labels):\n",
    "    best_thr, best_val = 0.5, 0.0\n",
    "    for thr in np.linspace(0.05, 0.95, 37):\n",
    "        preds = (probs >= thr).astype(int)\n",
    "        val = f1_score(labels, preds, zero_division=0)\n",
    "        if val > best_val:\n",
    "            best_val, best_thr = val, thr\n",
    "    return best_thr, best_val\n",
    "\n",
    "# Determine the best threshold on validation set\n",
    "best_thr_final, best_f1_valid = find_best_threshold(val_probs_final, val_labels_final)\n",
    "print(f'Best validation F1 threshold: {best_thr_final:.3f} (F1={best_f1_valid:.3f})')\n",
    "\n",
    "# Apply the threshold to test set\n",
    "test_preds_adj_final = (test_probs_final >= best_thr_final).astype(int)\n",
    "acc_final = accuracy_score(test_labels_final, test_preds_adj_final)\n",
    "f1_final = f1_score(test_labels_final, test_preds_adj_final, zero_division=0)\n",
    "prec_final = precision_score(test_labels_final, test_preds_adj_final, zero_division=0)\n",
    "recall_final = recall_score(test_labels_final, test_preds_adj_final, zero_division=0)\n",
    "print('Final test metrics (Weighted BCE + optimised threshold - TransformerMIL):')\n",
    "print(f'  Accuracy: {acc_final:.3f}, F1: {f1_final:.3f}, Precision: {prec_final:.3f}, Recall: {recall_final:.3f}, AUC: {test_auc_final:.3f}')\n",
    "\n",
    "# Confusion matrix\n",
    "cm_final = confusion_matrix(test_labels_final.astype(int), test_preds_adj_final.astype(int), labels=[0,1])\n",
    "plt.figure(figsize=(4,3))\n",
    "sns.heatmap(\n",
    "    cm_final, annot=True, fmt='d', cmap='Blues',\n",
    "    xticklabels=['Genuine','Forged'], yticklabels=['Genuine','Forged']\n",
    ")\n",
    "plt.title(f'TransformerMIL Confusion Matrix (Thr={best_thr_final:.2f})')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC curve\n",
    "fpr_final, tpr_final, _ = roc_curve(test_labels_final, test_probs_final)\n",
    "auc_final_value = auc(fpr_final, tpr_final)\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(fpr_final, tpr_final, label=f'TransformerMIL (AUC={auc_final_value:.3f})')\n",
    "plt.plot([0,1],[0,1],'k--', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve – TransformerMIL Model')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
