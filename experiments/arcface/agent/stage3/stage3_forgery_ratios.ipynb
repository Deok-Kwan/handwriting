{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39044d96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T10:53:30.948564Z",
     "start_time": "2025-09-14T10:53:23.730205Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3090\n",
      "ğŸ”’ ëª¨ë“  ì‹œë“œë¥¼ 42ë¡œ ê³ ì •ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# í™˜ê²½ ì„¤ì •\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, roc_curve, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# GPU ì„¤ì •\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = os.getenv('MIL_STAGE3_GPU', '3')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "if device.type == 'cuda':\n",
    "    print('GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.')\n",
    "\n",
    "# ì‹œë“œ ê³ ì • ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜\n",
    "def seed_everything(seed=42):\n",
    "    \"\"\"ëª¨ë“  ë‚œìˆ˜ ìƒì„±ê¸° ì‹œë“œë¥¼ ê³ ì •í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    print(f\"ğŸ”’ ëª¨ë“  ì‹œë“œë¥¼ {seed}ë¡œ ê³ ì •ì™„ë£Œ\")\n",
    "\n",
    "# ì´ˆê¸° ì‹œë“œ ê³ ì •\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g68jqs8g95u",
   "metadata": {},
   "source": "# Stage 3 ìœ„ì¡° ë¹„ìœ¨ ì‹¤í—˜: ë‹¤ì–‘í•œ ìœ„ì¡° ë¹„ìœ¨ì—ì„œì˜ MIL ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„\n\nì´ ë…¸íŠ¸ë¶ì€ Stage 2ì—ì„œ ìƒì„±í•œ **ë‹¤ì–‘í•œ ìœ„ì¡° ë¹„ìœ¨**(5%, 10%, 20%, 30%, 50%) MIL Bag ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ëª¨ë¸ ì„±ëŠ¥ì„ ë¹„êµ ì‹¤í—˜í•©ë‹ˆë‹¤.\n\n**ì‹¤í—˜ ëª©í‘œ:**\n- ë‚®ì€ ìœ„ì¡° ë¹„ìœ¨(5%, 10%)ì—ì„œì˜ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€\n- ìœ„ì¡° íƒì§€ Recall ìœ ì§€ ëŠ¥ë ¥ ë¶„ì„ (ìœ„ì¡°ë¥¼ ë†“ì¹˜ì§€ ì•ŠëŠ” ëŠ¥ë ¥)\n- Matched vs Shift ëª¨ë“œ ì„±ëŠ¥ ë¹„êµ\n- ëª¨ë¸ë³„ ìœ„ì¡° ë¹„ìœ¨ ì ì‘ì„± ë¶„ì„\n\n**ì‹¤í—˜ ëª¨ë“œ:**\n1. **Matched ëª¨ë“œ**: ê° ë¹„ìœ¨(5/10/20/30%)ë³„ë¡œ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµ/í‰ê°€\n2. **Shift ëª¨ë“œ**: 30% ë¹„ìœ¨ë¡œ í•™ìŠµ â†’ ë‹¤ì–‘í•œ ë¹„ìœ¨(5/10/20/30%)ë¡œ í‰ê°€\n\n**í‰ê°€ ëª¨ë¸ (ì´ 5ê°œ):**\n- **AttentionMIL**: ê¸°ë³¸ attention mechanism\n- **GatedAttentionMIL**: Gateë¡œ ì¡°ì ˆë˜ëŠ” attention mechanism  \n- **DSMIL**: Dual-stream MIL (attention bag score + max instance score ê²°í•©)\n- **TransMIL**: Transformer ê¸°ë°˜ MIL (1D PPEG, í‘œì¤€ MultiheadAttention)\n- **MeanPooling**: ë² ì´ìŠ¤ë¼ì¸ (ë‹¨ìˆœ í‰ê· )\n\n**ì¤‘ìš”í•œ ì—°êµ¬ ì§ˆë¬¸:**\n- ìœ„ì¡° ë¹„ìœ¨ì´ ë‚®ì•„ì§ˆìˆ˜ë¡ ì„±ëŠ¥ì´ ì–¼ë§ˆë‚˜ ì €í•˜ë˜ëŠ”ê°€?\n- ì–´ë–¤ ëª¨ë¸ì´ ë‚®ì€ ìœ„ì¡° ë¹„ìœ¨ì—ì„œ ë” ê°•ê±´í•œê°€?\n- 30% í•™ìŠµ ëª¨ë¸ì´ 5% í…ŒìŠ¤íŠ¸ì—ì„œë„ ì˜ ë™ì‘í•˜ëŠ”ê°€? (ë„ë©”ì¸ ì ì‘)\n- Transformer ê¸°ë°˜ ëª¨ë¸(TransMIL)ì´ ì „í†µì ì¸ MIL ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•œê°€?"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "data_loader_utils",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T10:53:30.980747Z",
     "start_time": "2025-09-14T10:53:30.954412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë°ì´í„° ë¡œë“œ ìœ í‹¸ë¦¬í‹° ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ìœ„ì¡° ë¹„ìœ¨ë³„ ë°ì´í„° ë¡œë“œ ìœ í‹¸ë¦¬í‹°\n",
    "def ratio_to_tag(pos_ratio: float) -> str:\n",
    "    \"\"\"ë¹„ìœ¨ì„ íŒŒì¼ëª… íƒœê·¸ë¡œ ë³€í™˜: 0.05 â†’ '05p', 0.30 â†’ '30p'\"\"\"\n",
    "    return f\"{int(round(pos_ratio*100)):02d}p\"\n",
    "\n",
    "def load_forgery_data(bags_dir, margin='0.4', ratios=[0.05, 0.10, 0.20, 0.30, 0.50]):\n",
    "    \"\"\"ë‹¤ì–‘í•œ ìœ„ì¡° ë¹„ìœ¨ì˜ MIL Bag ë°ì´í„°ë¥¼ ë¡œë“œ\"\"\"\n",
    "    data_dict = {}\n",
    "    \n",
    "    print(\"ğŸ“ ìœ„ì¡° ë¹„ìœ¨ë³„ ë°ì´í„° ë¡œë”© ì¤‘...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for ratio in ratios:\n",
    "        rtag = ratio_to_tag(ratio)\n",
    "        print(f\"  Loading {rtag} ({ratio:.0%}) datasets...\")\n",
    "        \n",
    "        # íŒŒì¼ ê²½ë¡œ ìƒì„±\n",
    "        train_pkl = os.path.join(bags_dir, f'bags_arcface_margin_{margin}_{rtag}_random_train.pkl')\n",
    "        val_pkl = os.path.join(bags_dir, f'bags_arcface_margin_{margin}_{rtag}_random_val.pkl')\n",
    "        test_pkl = os.path.join(bags_dir, f'bags_arcface_margin_{margin}_{rtag}_random_test.pkl')\n",
    "        \n",
    "        # ë°ì´í„° ë¡œë“œ\n",
    "        try:\n",
    "            with open(train_pkl, 'rb') as f:\n",
    "                train_data = pickle.load(f)\n",
    "            with open(val_pkl, 'rb') as f:\n",
    "                val_data = pickle.load(f)\n",
    "            with open(test_pkl, 'rb') as f:\n",
    "                test_data = pickle.load(f)\n",
    "                \n",
    "            data_dict[rtag] = {\n",
    "                'train': train_data,\n",
    "                'val': val_data,\n",
    "                'test': test_data,\n",
    "                'ratio': ratio\n",
    "            }\n",
    "            \n",
    "            print(f\"    âœ… {rtag}: Train={len(train_data['labels'])}, Val={len(val_data['labels'])}, Test={len(test_data['labels'])}\")\n",
    "            \n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"    âŒ {rtag}: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"âœ… ì´ {len(data_dict)}ê°œ ë¹„ìœ¨ ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ\")\n",
    "    return data_dict\n",
    "\n",
    "def load_shift_data(bags_dir, margin='0.4', train_ratio=0.30, eval_ratios=[0.05, 0.10, 0.20, 0.30, 0.50]):\n",
    "    \"\"\"Shift ëª¨ë“œìš© ë°ì´í„° ë¡œë“œ: 30% í•™ìŠµ + ë‹¤ì–‘í•œ ë¹„ìœ¨ í‰ê°€\"\"\"\n",
    "    print(\"ğŸ“ Shift ëª¨ë“œ ë°ì´í„° ë¡œë”© ì¤‘...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # í•™ìŠµìš© ë°ì´í„° (30%)\n",
    "    train_rtag = ratio_to_tag(train_ratio)\n",
    "    train_pkl = os.path.join(bags_dir, f'bags_arcface_margin_{margin}_{train_rtag}_random_train.pkl')\n",
    "    val_pkl = os.path.join(bags_dir, f'bags_arcface_margin_{margin}_{train_rtag}_random_val.pkl')\n",
    "    # Shift ì „ìš© ë„¤ì„ìŠ¤í˜ì´ìŠ¤ íŒŒì¼ì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©\n",
    "    train_pkl_shift = os.path.join(bags_dir, f'bags_arcface_margin_{margin}_{train_rtag}_random_train_shiftbase.pkl')\n",
    "    val_pkl_shift   = os.path.join(bags_dir, f'bags_arcface_margin_{margin}_{train_rtag}_random_val_shiftbase.pkl')\n",
    "    if os.path.exists(train_pkl_shift):\n",
    "        train_pkl = train_pkl_shift\n",
    "    if os.path.exists(val_pkl_shift):\n",
    "        val_pkl = val_pkl_shift\n",
    "\n",
    "\n",
    "    \n",
    "    with open(train_pkl, 'rb') as f:\n",
    "        train_data = pickle.load(f)\n",
    "    with open(val_pkl, 'rb') as f:\n",
    "        val_data = pickle.load(f)\n",
    "    \n",
    "    print(f\"  í•™ìŠµìš© {train_rtag} ({train_ratio:.0%}): Train={len(train_data['labels'])}, Val={len(val_data['labels'])}\")\n",
    "    \n",
    "    # í‰ê°€ìš© ë°ì´í„°ë“¤\n",
    "    eval_data = {}\n",
    "    for ratio in eval_ratios:\n",
    "        rtag = ratio_to_tag(ratio)\n",
    "        test_pkl = os.path.join(bags_dir, f'bags_arcface_margin_{margin}_{rtag}_random_test_shift_{rtag}.pkl')\n",
    "        \n",
    "        # Shift í‰ê°€ íŒŒì¼ì´ ì—†ìœ¼ë©´ ì¼ë°˜ í…ŒìŠ¤íŠ¸ íŒŒì¼ ì‚¬ìš©\n",
    "        if not os.path.exists(test_pkl):\n",
    "            test_pkl = os.path.join(bags_dir, f'bags_arcface_margin_{margin}_{rtag}_random_test.pkl')\n",
    "        \n",
    "        try:\n",
    "            with open(test_pkl, 'rb') as f:\n",
    "                test_data = pickle.load(f)\n",
    "            eval_data[rtag] = {\n",
    "                'test': test_data,\n",
    "                'ratio': ratio\n",
    "            }\n",
    "            print(f\"  í‰ê°€ìš© {rtag} ({ratio:.0%}): Test={len(test_data['labels'])}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"  âŒ {rtag}: í‰ê°€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"âœ… Shift ëª¨ë“œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: í•™ìŠµ {train_rtag} + í‰ê°€ {len(eval_data)}ê°œ\")\n",
    "    \n",
    "    return {\n",
    "        'train': train_data,\n",
    "        'val': val_data,\n",
    "        'eval_sets': eval_data,\n",
    "        'train_ratio': train_ratio\n",
    "    }\n",
    "\n",
    "print(\"âœ… ë°ì´í„° ë¡œë“œ ìœ í‹¸ë¦¬í‹° ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4742eb10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T10:53:36.819752Z",
     "start_time": "2025-09-14T10:53:30.982857Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ìœ„ì¡° ë¹„ìœ¨ë³„ ë°ì´í„° ë¡œë”© ì¤‘...\n",
      "============================================================\n",
      "  Loading 05p (5%) datasets...\n",
      "    âœ… 05p: Train=3600, Val=1200, Test=1200\n",
      "  Loading 10p (10%) datasets...\n",
      "    âœ… 10p: Train=3600, Val=1200, Test=1200\n",
      "  Loading 20p (20%) datasets...\n",
      "    âœ… 20p: Train=3600, Val=1200, Test=1200\n",
      "  Loading 30p (30%) datasets...\n",
      "    âœ… 30p: Train=3600, Val=1200, Test=1200\n",
      "  Loading 50p (50%) datasets...\n",
      "    âœ… 50p: Train=3600, Val=1200, Test=1200\n",
      "============================================================\n",
      "âœ… ì´ 5ê°œ ë¹„ìœ¨ ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ\n",
      "ğŸ“ Shift ëª¨ë“œ ë°ì´í„° ë¡œë”© ì¤‘...\n",
      "============================================================\n",
      "  í•™ìŠµìš© 30p (30%): Train=3600, Val=1200\n",
      "  í‰ê°€ìš© 05p (5%): Test=1200\n",
      "  í‰ê°€ìš© 10p (10%): Test=1200\n",
      "  í‰ê°€ìš© 20p (20%): Test=1200\n",
      "  í‰ê°€ìš© 30p (30%): Test=1200\n",
      "  í‰ê°€ìš© 50p (50%): Test=1200\n",
      "============================================================\n",
      "âœ… Shift ëª¨ë“œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: í•™ìŠµ 30p + í‰ê°€ 5ê°œ\n",
      "\n",
      "ğŸ”„ Instance mean ê³„ì‚° ì¤‘...\n",
      "âœ… ëª¨ë“  ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ìœ„ì¡° ë¹„ìœ¨ë³„ ë°ì´í„° ë¡œë“œ\n",
    "embedding_margin = '0.4'\n",
    "bags_dir = '/workspace/MIL/data/processed/bags'\n",
    "RATIOS = [0.05, 0.10, 0.20, 0.30, 0.50]\n",
    "\n",
    "# Matched ëª¨ë“œìš© ë°ì´í„° ë¡œë“œ\n",
    "matched_data = load_forgery_data(bags_dir, embedding_margin, RATIOS)\n",
    "\n",
    "# Shift ëª¨ë“œìš© ë°ì´í„° ë¡œë“œ\n",
    "shift_data = load_shift_data(bags_dir, embedding_margin, train_ratio=0.30, eval_ratios=RATIOS)\n",
    "\n",
    "# Instance mean ê³„ì‚° í•¨ìˆ˜: (10,5,256) â†’ (10,256)\n",
    "def to_instance_means(bags):\n",
    "    return [bag.mean(axis=1).astype(np.float32) for bag in bags]\n",
    "\n",
    "print(\"\\nğŸ”„ Instance mean ê³„ì‚° ì¤‘...\")\n",
    "# Matched ë°ì´í„° ì „ì²˜ë¦¬\n",
    "for rtag, data in matched_data.items():\n",
    "    data['train_features'] = to_instance_means(data['train']['bags'])\n",
    "    data['val_features'] = to_instance_means(data['val']['bags'])\n",
    "    data['test_features'] = to_instance_means(data['test']['bags'])\n",
    "    \n",
    "# Shift ë°ì´í„° ì „ì²˜ë¦¬\n",
    "shift_data['train_features'] = to_instance_means(shift_data['train']['bags'])\n",
    "shift_data['val_features'] = to_instance_means(shift_data['val']['bags'])\n",
    "for rtag, eval_set in shift_data['eval_sets'].items():\n",
    "    eval_set['test_features'] = to_instance_means(eval_set['test']['bags'])\n",
    "\n",
    "print(\"âœ… ëª¨ë“  ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17dbf94a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T10:53:36.831563Z",
     "start_time": "2025-09-14T10:53:36.823803Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset í´ë˜ìŠ¤ ë° DataLoader ìœ í‹¸ë¦¬í‹° ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# Dataset í´ë˜ìŠ¤ (onâ€‘theâ€‘fly Tensor ë³€í™˜)\n",
    "class MILDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features  # list of np.ndarray\n",
    "        self.labels = labels      # list of int\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return x, y\n",
    "\n",
    "def create_dataloaders(features, labels, batch_size=16):\n",
    "    \"\"\"Featuresì™€ labelsë¡œë¶€í„° DataLoader ìƒì„±\"\"\"\n",
    "    train_loader = DataLoader(MILDataset(features['train'], labels['train']), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(MILDataset(features['val'], labels['val']), batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(MILDataset(features['test'], labels['test']), batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "print(\"âœ… Dataset í´ë˜ìŠ¤ ë° DataLoader ìœ í‹¸ë¦¬í‹° ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "qpxriysblvi",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T10:53:36.840044Z",
     "start_time": "2025-09-14T10:53:36.833780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… WeightedBCE ì†ì‹¤í•¨ìˆ˜ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\n",
      "   FP Weight: 2.0 (baselineê³¼ ë™ì¼)\n"
     ]
    }
   ],
   "source": [
    "# WeightedBCE ì†ì‹¤í•¨ìˆ˜ ì •ì˜\n",
    "class WeightedBCE(nn.Module):\n",
    "    \"\"\"Weighted Binary Cross Entropy Loss - False Positiveì— ë” í° ê°€ì¤‘ì¹˜ ë¶€ì—¬\"\"\"\n",
    "    def __init__(self, fp_weight=2.0):\n",
    "        super().__init__()\n",
    "        self.fp_weight = fp_weight\n",
    "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
    "    \n",
    "    def forward(self, logits, labels):\n",
    "        loss = self.bce(logits, labels)\n",
    "        # False Positive (label=0ì¸ë° ì˜ˆì¸¡ì´ 1)ì— ë” í° ê°€ì¤‘ì¹˜\n",
    "        fp_mask = (labels == 0).float()\n",
    "        loss = loss * (1 + self.fp_weight * fp_mask)\n",
    "        return loss.mean()\n",
    "\n",
    "print(\"âœ… WeightedBCE ì†ì‹¤í•¨ìˆ˜ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")\n",
    "print(f\"   FP Weight: 2.0 (baselineê³¼ ë™ì¼)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79407013",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T10:53:36.864074Z",
     "start_time": "2025-09-14T10:53:36.842802Z"
    }
   },
   "outputs": [],
   "source": "# MIL ëª¨ë¸ ì •ì˜: Attention vs DSMIL vs Gated vs Mean Pooling vs TransMIL\n\nclass AttentionMIL(nn.Module):\n    \"\"\"ê¸°ë³¸ Attention-based MIL ëª¨ë¸\"\"\"\n    def __init__(self, input_dim=256, hidden_dim=128, dropout_p=0.1):\n        super().__init__()\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        \n        # Instance-level feature transformation\n        self.instance_fc = nn.Linear(input_dim, hidden_dim)\n        self.dropout = nn.Dropout(dropout_p)\n        \n        # Attention mechanism\n        self.att_fc1 = nn.Linear(hidden_dim, hidden_dim)\n        self.att_fc2 = nn.Linear(hidden_dim, 1)\n        \n        # Classifier\n        self.classifier = nn.Linear(hidden_dim, 1)\n        self._init_weights()\n    \n    def _init_weights(self):\n        # He initialization for ReLU layers\n        nn.init.kaiming_uniform_(self.instance_fc.weight, nonlinearity='relu')\n        nn.init.zeros_(self.instance_fc.bias)\n        # Xavier for others\n        nn.init.xavier_uniform_(self.att_fc1.weight)\n        nn.init.zeros_(self.att_fc1.bias)\n        nn.init.xavier_uniform_(self.att_fc2.weight)\n        nn.init.zeros_(self.att_fc2.bias)\n        nn.init.xavier_uniform_(self.classifier.weight)\n        nn.init.zeros_(self.classifier.bias)\n    \n    def forward(self, x):\n        # Instance feature transformation: (B, N, input_dim) -> (B, N, hidden_dim)\n        h = torch.relu(self.instance_fc(x))\n        h = self.dropout(h)\n        \n        # Attention calculation\n        a = torch.tanh(self.att_fc1(h))  # (B, N, hidden_dim)\n        a = self.att_fc2(a).squeeze(-1)  # (B, N)\n        weights = torch.softmax(a, dim=1)  # (B, N)\n        \n        # Weighted aggregation\n        bag_repr = torch.sum(weights.unsqueeze(-1) * h, dim=1)  # (B, hidden_dim)\n        bag_repr = self.dropout(bag_repr)\n        \n        # Classification\n        logits = self.classifier(bag_repr).squeeze(-1)  # (B,)\n        return logits, weights\n\nclass GatedAttentionMIL(nn.Module):\n    \"\"\"Gated Attention MIL ëª¨ë¸ - Gate mechanismìœ¼ë¡œ attention ì¡°ì ˆ\"\"\"\n    def __init__(self, input_dim=256, hidden_dim=128, dropout_p=0.1):\n        super().__init__()\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        \n        # Instance-level feature transformation\n        self.instance_fc = nn.Linear(input_dim, hidden_dim)\n        self.dropout = nn.Dropout(dropout_p)\n        \n        # Attention branch\n        self.attention_fc1 = nn.Linear(hidden_dim, hidden_dim)\n        self.attention_fc2 = nn.Linear(hidden_dim, 1)\n        \n        # Gate branch  \n        self.gate_fc1 = nn.Linear(hidden_dim, hidden_dim)\n        \n        # Classifier\n        self.classifier = nn.Linear(hidden_dim, 1)\n        self._init_weights()\n    \n    def _init_weights(self):\n        # He initialization for ReLU layers\n        nn.init.kaiming_uniform_(self.instance_fc.weight, nonlinearity='relu')\n        nn.init.zeros_(self.instance_fc.bias)\n        \n        # Xavier for attention branch\n        nn.init.xavier_uniform_(self.attention_fc1.weight)\n        nn.init.zeros_(self.attention_fc1.bias)\n        nn.init.xavier_uniform_(self.attention_fc2.weight)\n        nn.init.zeros_(self.attention_fc2.bias)\n        \n        # Xavier for gate branch\n        nn.init.xavier_uniform_(self.gate_fc1.weight)\n        nn.init.zeros_(self.gate_fc1.bias)\n        \n        # Classifier\n        nn.init.xavier_uniform_(self.classifier.weight)\n        nn.init.zeros_(self.classifier.bias)\n    \n    def forward(self, x):\n        # Instance feature transformation: (B, N, input_dim) -> (B, N, hidden_dim)\n        h = torch.relu(self.instance_fc(x))\n        h = self.dropout(h)\n        \n        # Attention branch\n        attention = torch.tanh(self.attention_fc1(h))  # (B, N, hidden_dim)\n        \n        # Gate branch (vector gate)\n        gate = torch.sigmoid(self.gate_fc1(h))  # (B, N, hidden_dim) - vector gate\n        \n        # Combine attention with gate (canonical: vector-level gating)\n        scores = self.attention_fc2(attention * gate).squeeze(-1)  # (B, N)\n        \n        # Softmax normalization\n        weights = torch.softmax(scores, dim=1)  # (B, N)\n        \n        # Weighted aggregation\n        bag_repr = torch.sum(weights.unsqueeze(-1) * h, dim=1)  # (B, hidden_dim)\n        bag_repr = self.dropout(bag_repr)\n        \n        # Classification\n        logits = self.classifier(bag_repr).squeeze(-1)  # (B,)\n        return logits, weights\n\nclass DSMILModel(nn.Module):\n    \"\"\"DSMIL (ì› ë…¼ë¬¸ ì •í•© ë²„ì „)\n    - instance branch: inst_logits (B, N)\n    - aggregator branch: critical instance ê¸°ë°˜ cross-attentionìœ¼ë¡œ bag_logit_attn (B,)\n    - final: bag_logits = alpha * bag_logit_attn + (1 - alpha) * max_inst_logit\n    ë°˜í™˜: bag_logits, attn_w, inst_logits, top_idx\n    \"\"\"\n    def __init__(self, input_dim=256, hidden_dim=128, dropout_p=0.1, alpha=0.5, temperature=None):\n        super().__init__()\n        self.enc = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout_p)\n        )\n        # instance classifier (instance branch)\n        self.inst_cls = nn.Linear(hidden_dim, 1)\n\n        # DSMIL aggregator: single projection for queries + values\n        self.query_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.val_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.bag_cls = nn.Linear(hidden_dim, 1)\n\n        # dual-stream ê²°í•© íŒŒë¼ë¯¸í„°\n        self.alpha = alpha\n        self.temperature = temperature\n\n        # weight init\n        nn.init.xavier_uniform_(self.inst_cls.weight); nn.init.zeros_(self.inst_cls.bias)\n        nn.init.xavier_uniform_(self.query_proj.weight)\n        nn.init.xavier_uniform_(self.val_proj.weight)\n        nn.init.xavier_uniform_(self.bag_cls.weight); nn.init.zeros_(self.bag_cls.bias)\n\n    def forward(self, x):\n        # x: (B, N, D)\n        h = self.enc(x)                                # (B, N, H)\n        inst_logits = self.inst_cls(h).squeeze(-1)     # (B, N)\n\n        # critical instance (top-1 by logit)\n        top_idx = inst_logits.argmax(dim=1)            # (B,)\n        B, N, H = h.shape\n        batch = torch.arange(B, device=h.device)\n\n        # projector outputs\n        Q = self.query_proj(h)                         # (B, N, H)\n        q_star = Q[batch, top_idx]                     # (B, H)\n        attn_score = torch.bmm(Q, q_star.unsqueeze(-1)).squeeze(-1)  # (B, N)\n\n        scale = (H ** 0.5) if self.temperature is None else self.temperature\n        attn_w = torch.softmax(attn_score / scale, dim=1)           # (B, N)\n\n        V = self.val_proj(h)                           # (B, N, H)\n        attn_repr = (attn_w.unsqueeze(-1) * V).sum(dim=1)           # (B, H)\n        bag_logit_attn = self.bag_cls(attn_repr).squeeze(-1)        # (B,)\n\n        # dual-stream ê²°í•©: instance-score max\n        max_inst_logit = inst_logits.max(dim=1).values              # (B,)\n        bag_logits = self.alpha * bag_logit_attn + (1.0 - self.alpha) * max_inst_logit\n\n        return bag_logits, attn_w, inst_logits, top_idx\n\n\n# TransMIL ëª¨ë¸ êµ¬ì„±ìš”ì†Œë“¤ (stage3_baseline_transmil.ipynbì—ì„œ ì •í™•íˆ ë³µì‚¬)\n\nclass TransBlock(nn.Module):\n    \"\"\"í‘œì¤€ Transformer ë¸”ë¡ (FFN í¬í•¨)\"\"\"\n    def __init__(self, dim=512, num_heads=8, dropout_p=0.1, ffn_mult=4):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(dim)\n        self.attn = nn.MultiheadAttention(\n            embed_dim=dim, num_heads=num_heads, dropout=dropout_p, batch_first=True\n        )\n        self.norm2 = nn.LayerNorm(dim)\n        self.ffn = nn.Sequential(\n            nn.Linear(dim, ffn_mult * dim),\n            nn.GELU(),  # ViT ê³„ì—´ì€ GELUê°€ ë³´í¸ì \n            nn.Dropout(dropout_p),\n            nn.Linear(ffn_mult * dim, dim),\n            nn.Dropout(dropout_p),\n        )\n    \n    def forward(self, x):\n        # Pre-norm style\n        normed_x = self.norm1(x)\n        attn_out, _ = self.attn(normed_x, normed_x, normed_x, need_weights=False)\n        x = x + attn_out\n        \n        ffn_out = self.ffn(self.norm2(x))\n        x = x + ffn_out\n        return x\n\n\nclass PPEG1D(nn.Module):\n    \"\"\"1D PPEG (depthwise Conv1d)\"\"\"\n    def __init__(self, dim=512):\n        super().__init__()\n        self.proj7 = nn.Conv1d(dim, dim, kernel_size=7, padding=3, groups=dim)\n        self.proj5 = nn.Conv1d(dim, dim, kernel_size=5, padding=2, groups=dim)\n        self.proj3 = nn.Conv1d(dim, dim, kernel_size=3, padding=1, groups=dim)\n\n    def forward(self, x):\n        # x: (B, 1+N, C) = [CLS | tokens]\n        cls_token, feat_token = x[:, :1], x[:, 1:]  # (B,1,C), (B,N,C)\n        b, n, c = feat_token.shape\n        feat = feat_token.transpose(1, 2)  # (B,C,N)\n        feat = self.proj7(feat) + self.proj5(feat) + self.proj3(feat) + feat\n        feat = feat.transpose(1, 2)  # (B,N,C)\n        return torch.cat([cls_token, feat], dim=1)  # (B,1+N,C)\n\n\nclass LearnablePosEmb1D(nn.Module):\n    \"\"\"ê°„ë‹¨í•œ í•™ìŠµí˜• 1D ìœ„ì¹˜ì„ë² ë”©\"\"\"\n    def __init__(self, max_len=512, dim=512):\n        super().__init__()\n        self.pos = nn.Parameter(torch.zeros(1, max_len, dim))\n        nn.init.trunc_normal_(self.pos, std=0.02)\n    \n    def forward(self, x):\n        # x: (B, 1+N, C)\n        return x + self.pos[:, :x.size(1), :]\n\n\nclass TransMIL(nn.Module):\n    \"\"\"ìˆ˜ì •ëœ TransMIL - 1D ì‹œí€€ìŠ¤ìš©ìœ¼ë¡œ ì¬ì„¤ê³„\"\"\"\n    def __init__(self, input_dim=256, embed_dim=512, num_heads=8, dropout_p=0.1, n_classes=1,\n                 use_1d_ppeg=True, max_len=512):\n        super().__init__()\n        assert embed_dim % num_heads == 0\n\n        self.embed = nn.Sequential(\n            nn.Linear(input_dim, embed_dim),\n            nn.GELU(),  # ReLU â†’ GELU ê¶Œì¥\n            nn.Dropout(dropout_p),\n        )\n        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim) * 0.02)\n\n        # ë‘ ê°œì˜ Transformer ë¸”ë¡\n        self.block1 = TransBlock(embed_dim, num_heads, dropout_p)\n        self.block2 = TransBlock(embed_dim, num_heads, dropout_p)\n\n        # ìœ„ì¹˜ ë¶€ì—¬: 1D PPEG ë˜ëŠ” learnable pos emb\n        if use_1d_ppeg:\n            self.pos_layer = PPEG1D(embed_dim)\n            self.use_learnable_pos = False\n        else:\n            self.pos_layer = LearnablePosEmb1D(max_len=max_len, dim=embed_dim)\n            self.use_learnable_pos = True\n\n        self.norm = nn.LayerNorm(embed_dim)\n        self.classifier = nn.Linear(embed_dim, n_classes)\n        self._init_weights()\n\n    def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, (nn.Conv1d, nn.Conv2d)):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        \"\"\"\n        x: (B, N, D_in)  # N = ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ (ì •ì‚¬ê° íŒ¨ë”© ì—†ìŒ)\n        \"\"\"\n        assert x.dim() == 3, 'Input must be (batch, instances, features).'\n        h = self.embed(x)  # (B,N,C)\n        b, n, c = h.shape\n        cls = self.cls_token.expand(b, 1, c)  # (B,1,C)\n        h = torch.cat([cls, h], dim=1)  # (B,1+N,C)\n\n        # ìœ„ì¹˜ë¶€ì—¬\n        if self.use_learnable_pos:\n            h = self.pos_layer(h)  # learnable pos emb\n            h = self.block1(h)\n            h = self.block2(h)\n        else:\n            h = self.block1(h)  # Pre-attnë¡œ ì•½ê°„ ì„ì€ ë’¤\n            h = self.pos_layer(h)  # 1D PPEG\n            h = self.block2(h)\n\n        h = self.norm(h)\n        cls_out = h[:, 0]  # (B,C)\n        logits = self.classifier(cls_out)  # (B,1)\n        return logits.squeeze(-1)\n\n\nclass ClassWeightedBCE(nn.Module):\n    \"\"\"ëª…í™•í•œ í´ë˜ìŠ¤ ê°€ì¤‘ BCE\"\"\"\n    def __init__(self, pos_weight=1.0, neg_weight=1.0):\n        super().__init__()\n        self.pos_weight = pos_weight\n        self.neg_weight = neg_weight\n    \n    def forward(self, logits, labels):\n        # labels: {0,1} float tensor\n        weights = labels * self.pos_weight + (1.0 - labels) * self.neg_weight\n        return nn.functional.binary_cross_entropy_with_logits(\n            logits, labels, weight=weights, reduction='mean'\n        )\n\n\nclass MeanPoolingModel(nn.Module):\n    \"\"\"ë² ì´ìŠ¤ë¼ì¸: ë‹¨ìˆœ í‰ê·  í’€ë§ ëª¨ë¸\"\"\"\n    def __init__(self, input_dim=256):\n        super().__init__()\n        self.fc = nn.Linear(input_dim, 1)\n        nn.init.xavier_uniform_(self.fc.weight)\n        nn.init.zeros_(self.fc.bias)\n    \n    def forward(self, x):\n        bag_mean = x.mean(dim=1)  # (B, input_dim)\n        logits = self.fc(bag_mean).squeeze(-1)  # (B,)\n        return logits\n\nprint(\"âœ… ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ:\")\nprint(\"  - AttentionMIL: ê¸°ë³¸ attention mechanism\")\nprint(\"  - GatedAttentionMIL: Gateë¡œ ì¡°ì ˆë˜ëŠ” attention mechanism\")\nprint(\"  - DSMILModel: Dual-stream MIL (attention bag score + max instance score í‰ê· )\")\nprint(\"  - TransMIL: Transformer ê¸°ë°˜ MIL (1D PPEG, í‘œì¤€ MultiheadAttention)\")\nprint(\"  - MeanPoolingModel: ë² ì´ìŠ¤ë¼ì¸ (ë‹¨ìˆœ í‰ê· )\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bb9c1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T10:53:36.883930Z",
     "start_time": "2025-09-14T10:53:36.866378Z"
    }
   },
   "outputs": [],
   "source": "# ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ í•¨ìˆ˜\nfrom sklearn.metrics import precision_score, recall_score\n\ndef is_attention_model(model):\n    \"\"\"Attention ê¸°ë°˜ ëª¨ë¸ì¸ì§€ í™•ì¸ (AttentionMIL, GatedAttentionMIL, DSMILModel)\"\"\"\n    return isinstance(model, (AttentionMIL, GatedAttentionMIL, DSMILModel))\n\ndef _extract_logits(model, X):\n    \"\"\"ëª¨ë¸ ì¶œë ¥ì—ì„œ logits ì¶”ì¶œ (TransMIL ì§€ì›)\"\"\"\n    output = model(X)\n    if isinstance(output, dict):\n        if 'logits' not in output:\n            raise KeyError('Model output dictionary must contain a `logits` key.')\n        logits = output['logits']\n    elif isinstance(output, tuple):\n        logits = output[0]\n    else:\n        logits = output\n    if isinstance(logits, (list, tuple)):\n        logits = logits[0]\n    if hasattr(logits, 'dim') and logits.dim() == 2 and logits.size(-1) == 1:\n        logits = logits.squeeze(-1)\n    return logits\n\ndef train_one_epoch(model, optimizer, loader, bag_criterion, inst_loss_weight=0.5):\n    model.train()\n    total_loss, preds_all, labels_all = 0.0, [], []\n    inst_criterion = nn.BCEWithLogitsLoss()\n    for X, y in tqdm(loader, desc='Train', leave=False):\n        X, y = X.to(device), y.to(device)\n        optimizer.zero_grad()\n        out = model(X)\n        \n        # TransMIL ëª¨ë¸ì˜ ê²½ìš° ë‹¨ì¼ logits ë°˜í™˜\n        if isinstance(model, TransMIL):\n            logits = out\n            loss = bag_criterion(logits, y)\n        elif isinstance(out, (tuple, list)) and len(out) == 4:\n            logits, _, inst_logits, top_idx = out\n            bag_loss = bag_criterion(logits, y)\n            if inst_loss_weight > 0:\n                batch_idx = torch.arange(y.size(0), device=y.device)\n                top_inst_logits = inst_logits[batch_idx, top_idx]\n                inst_loss = inst_criterion(top_inst_logits, y)\n                loss = bag_loss + inst_loss_weight * inst_loss\n            else:\n                loss = bag_loss\n        else:\n            logits = out[0] if isinstance(out, (tuple, list)) else out\n            loss = bag_criterion(logits, y)\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        total_loss += loss.item() * y.size(0)\n        preds_all.extend((torch.sigmoid(logits) >= 0.5).float().cpu().numpy())\n        labels_all.extend(y.cpu().numpy())\n    return total_loss / len(loader.dataset), accuracy_score(labels_all, preds_all)\n\ndef evaluate(model, loader, criterion):\n    model.eval()\n    total_loss = 0.0\n    probs_all, preds_all, labels_all = [], [], []\n    attention_weights_all = []\n\n    with torch.no_grad():\n        for X, y in tqdm(loader, desc='Eval', leave=False):\n            X, y = X.to(device), y.to(device)\n            out = model(X)\n            \n            # TransMIL ëª¨ë¸ì˜ ê²½ìš° ë‹¨ì¼ logits ë°˜í™˜\n            if isinstance(model, TransMIL):\n                logits = out\n            elif isinstance(out, (tuple, list)):\n                logits = out[0]\n                if len(out) >= 2 and out[1] is not None:\n                    attention_weights_all.append(out[1].detach().cpu().numpy())\n            else:\n                logits = out\n                \n            loss = criterion(logits, y)\n            total_loss += loss.item() * y.size(0)\n            probs = torch.sigmoid(logits)\n            preds = (probs >= 0.5).float()\n            probs_all.extend(probs.cpu().numpy())\n            preds_all.extend(preds.cpu().numpy())\n            labels_all.extend(y.cpu().numpy())\n\n    acc = accuracy_score(labels_all, preds_all)\n    auc = roc_auc_score(labels_all, probs_all) if len(set(labels_all)) > 1 else 0.0\n    f1 = f1_score(labels_all, preds_all) if len(set(preds_all)) > 1 else 0.0\n    precision = precision_score(labels_all, preds_all, zero_division=0.0)\n    recall = recall_score(labels_all, preds_all, zero_division=0.0)\n\n    attention_weights_combined = np.concatenate(attention_weights_all, axis=0) if attention_weights_all else None\n\n    return {\n        'loss': total_loss / len(loader.dataset),\n        'accuracy': acc,\n        'auc': auc,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall,\n        'probs': np.array(probs_all),\n        'labels': np.array(labels_all),\n        'preds': np.array(preds_all),\n        'attention_weights': attention_weights_combined\n    }\n\ndef train_model(model, optimizer, scheduler, train_loader, val_loader, criterion, \n                max_epochs=10, patience=3, name='model', inst_loss_weight=0.0):\n    \"\"\"ëª¨ë¸ í•™ìŠµ (Early Stopping í¬í•¨)\"\"\"\n    best_auc = 0.0\n    best_state = None\n    epochs_no_improve = 0\n    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'val_auc': [], 'val_f1': []}\n\n    print(f\"\\nğŸš€ {name} ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n    print(f\"   Max epochs: {max_epochs}, Patience: {patience}\")\n\n    for epoch in range(1, max_epochs + 1):\n        print(f\"\\nEpoch {epoch}/{max_epochs} â€“ {name}\")\n        tr_loss, tr_acc = train_one_epoch(model, optimizer, train_loader, criterion, inst_loss_weight=inst_loss_weight)\n        val_results = evaluate(model, val_loader, criterion)\n        val_loss, val_acc, val_auc, val_f1 = val_results['loss'], val_results['accuracy'], val_results['auc'], val_results['f1']\n\n        history['train_loss'].append(tr_loss)\n        history['train_acc'].append(tr_acc)\n        history['val_loss'].append(val_loss)\n        history['val_acc'].append(val_acc)\n        history['val_auc'].append(val_auc)\n        history['val_f1'].append(val_f1)\n\n        print(f\"  Train: Loss={tr_loss:.4f}, Acc={tr_acc:.4f}\")\n        print(f\"  Val:   Loss={val_loss:.4f}, Acc={val_acc:.4f}, AUC={val_auc:.4f}, F1={val_f1:.4f}\")\n\n        scheduler.step(val_auc)\n\n        if val_auc > best_auc:\n            best_auc = val_auc\n            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n            torch.save(best_state, f'best_{name}.pth')\n            print(f\"  âœ… New best AUC: {best_auc:.4f} â€“ model saved.\")\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n            print(f\"  â³ No improvement. Patience: {epochs_no_improve}/{patience}\")\n            if epochs_no_improve >= patience:\n                print(\"  ğŸ›‘ Early stopping triggered.\")\n                break\n\n    if best_state is not None:\n        model.load_state_dict(best_state)\n        print(f\"  ğŸ“‚ Best model loaded (AUC: {best_auc:.4f})\")\n\n    return model, history\n\nprint(\"âœ… í•™ìŠµ/í‰ê°€ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ:\")\nprint(\"  - ë‹¤ì¤‘ ëª¨ë¸ ì§€ì› (Attention/Gated/DSMIL/TransMIL/MeanPooling)\")\nprint(\"  - TransMIL ë‹¨ì¼ logits ì¶œë ¥ ì§€ì›\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched_experiments",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T10:56:02.092591Z",
     "start_time": "2025-09-14T10:53:36.886201Z"
    }
   },
   "outputs": [],
   "source": "# ==============================================================================\n# Matched ëª¨ë“œ ì‹¤í—˜: ê° ë¹„ìœ¨ë³„ë¡œ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµ/í‰ê°€\n# ==============================================================================\n\nprint(\"ğŸ”¬ Matched ëª¨ë“œ ì‹¤í—˜ ì‹œì‘\")\nprint(\"=\" * 80)\nprint(\"ê° ìœ„ì¡° ë¹„ìœ¨ë³„ë¡œ ë…ë¦½ì ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  í‰ê°€í•©ë‹ˆë‹¤.\")\nprint(\"ëª©í‘œ: ìœ„ì¡° ë¹„ìœ¨ ë³€í™”ì— ë”°ë¥¸ ëª¨ë¸ ì„±ëŠ¥ ë³€í™” ë¶„ì„\")\nprint(\"=\" * 80)\n\n# ì‹¤í—˜ ì„¤ì •\ncriterion = WeightedBCE(fp_weight=2.0)\nlearning_rate = 1e-3\nmax_epochs = 10\npatience = 3\nscheduler_patience = 1\nbatch_size = 16\n\n# ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ (TransMIL ì¶”ê°€ - ì´ 5ê°œ ëª¨ë¸)\nMODEL_CLASSES = {\n    'AttentionMIL': AttentionMIL,\n    'GatedAttentionMIL': GatedAttentionMIL,\n    'DSMIL': DSMILModel,\n    'TransMIL': TransMIL,\n    'MeanPooling': MeanPoolingModel\n}\n\n# ê²°ê³¼ ì €ì¥\nmatched_results = {}\n\n# ê° ë¹„ìœ¨ë³„ë¡œ ì‹¤í—˜\nfor rtag, data in matched_data.items():\n    ratio = data['ratio']\n    print(f\"\\n{'='*60}\")\n    print(f\"ğŸ¯ {rtag} ({ratio:.0%}) ìœ„ì¡° ë¹„ìœ¨ ì‹¤í—˜\")\n    print(f\"{'='*60}\")\n    \n    # DataLoader ìƒì„±\n    features = {\n        'train': data['train_features'],\n        'val': data['val_features'],\n        'test': data['test_features']\n    }\n    labels = {\n        'train': data['train']['labels'],\n        'val': data['val']['labels'],\n        'test': data['test']['labels']\n    }\n    \n    train_loader, val_loader, test_loader = create_dataloaders(features, labels, batch_size)\n    \n    matched_results[rtag] = {'ratio': ratio, 'models': {}}\n    \n    # ê° ëª¨ë¸ë³„ë¡œ í•™ìŠµ\n    for model_name, model_class in MODEL_CLASSES.items():\n        print(f\"\\nğŸš€ {model_name} í•™ìŠµ ì¤‘... (ë¹„ìœ¨: {rtag})\")\n        \n        # ì‹œë“œ ê³ ì • (ì¬í˜„ì„±)\n        seed_everything(42)\n        \n        # ëª¨ë¸ ìƒì„± (TransMIL íŠ¹ë³„ ì²˜ë¦¬)\n        if model_name == 'MeanPooling':\n            model = model_class(input_dim=256).to(device)\n        elif model_name == 'TransMIL':\n            model = model_class(\n                input_dim=256, \n                embed_dim=512, \n                num_heads=8, \n                dropout_p=0.1, \n                use_1d_ppeg=True\n            ).to(device)\n        else:\n            model = model_class(input_dim=256, hidden_dim=128, dropout_p=0.1).to(device)\n        \n        # Optimizer & Scheduler (TransMILì€ AdamW ì‚¬ìš©)\n        if model_name == 'TransMIL':\n            optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-2)\n        else:\n            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n        \n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, mode='max', factor=0.5, patience=scheduler_patience, verbose=False\n        )\n        \n        # í•™ìŠµ (TransMILì€ ClassWeightedBCE ì‚¬ìš©)\n        if model_name == 'TransMIL':\n            transmil_criterion = ClassWeightedBCE(pos_weight=1.0, neg_weight=2.0)\n            inst_weight = 0.0\n        elif model_name == 'DSMIL':\n            transmil_criterion = criterion\n            inst_weight = 0.5\n        else:\n            transmil_criterion = criterion\n            inst_weight = 0.0\n        \n        model, history = train_model(\n            model, optimizer, scheduler, train_loader, val_loader, transmil_criterion,\n            max_epochs=max_epochs, patience=patience, name=f'{model_name}_{rtag}',\n            inst_loss_weight=inst_weight\n        )\n        \n        # í‰ê°€\n        val_results = evaluate(model, val_loader, transmil_criterion)\n        test_results = evaluate(model, test_loader, transmil_criterion)\n        \n        # ê²°ê³¼ ì €ì¥\n        matched_results[rtag]['models'][model_name] = {\n            'model': model,\n            'history': history,\n            'val': val_results,\n            'test': test_results\n        }\n        \n        print(f\"  âœ… {model_name} ì™„ë£Œ: Val AUC={val_results['auc']:.3f}, Test AUC={test_results['auc']:.3f}\")\n        print(f\"     Test Recall={test_results['recall']:.3f}, Test F1={test_results['f1']:.3f}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"ğŸ† Matched ëª¨ë“œ ì‹¤í—˜ ì™„ë£Œ!\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shift_experiments",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T10:56:27.197224Z",
     "start_time": "2025-09-14T10:56:02.095511Z"
    }
   },
   "outputs": [],
   "source": "# ==============================================================================\n# Shift ëª¨ë“œ ì‹¤í—˜: 30% í•™ìŠµ â†’ ë‹¤ì–‘í•œ ë¹„ìœ¨ í‰ê°€\n# ==============================================================================\n\nprint(\"\\nğŸ”„ Shift ëª¨ë“œ ì‹¤í—˜ ì‹œì‘\")\nprint(\"=\" * 80)\nprint(\"30% ìœ„ì¡° ë¹„ìœ¨ë¡œ í•™ìŠµí•œ ëª¨ë¸ì„ ë‹¤ì–‘í•œ ë¹„ìœ¨(5/10/20/30/50%)ë¡œ í‰ê°€í•©ë‹ˆë‹¤.\")\nprint(\"ëª©í‘œ: ë„ë©”ì¸ ì ì‘ì„± ë° ì¼ë°˜í™” ëŠ¥ë ¥ ë¶„ì„\")\nprint(\"=\" * 80)\n\n# 30% í•™ìŠµ ë°ì´í„°ë¡œ DataLoader ìƒì„±\nshift_features = {\n    'train': shift_data['train_features'],\n    'val': shift_data['val_features']\n}\nshift_labels = {\n    'train': shift_data['train']['labels'],\n    'val': shift_data['val']['labels']\n}\n\nshift_train_loader = DataLoader(MILDataset(shift_features['train'], shift_labels['train']), \n                               batch_size=batch_size, shuffle=True)\nshift_val_loader = DataLoader(MILDataset(shift_features['val'], shift_labels['val']), \n                             batch_size=batch_size, shuffle=False)\n\nprint(f\"í•™ìŠµ ë°ì´í„°: 30% ìœ„ì¡° ë¹„ìœ¨, Train={len(shift_labels['train'])}, Val={len(shift_labels['val'])}\")\n\n# ê²°ê³¼ ì €ì¥\nshift_results = {'train_ratio': shift_data['train_ratio'], 'models': {}}\n\n# ê° ëª¨ë¸ë³„ë¡œ 30% ë°ì´í„°ë¡œ í•™ìŠµ (ì´ 5ê°œ ëª¨ë¸)\nfor model_name, model_class in MODEL_CLASSES.items():\n    print(f\"\\nğŸš€ {model_name} í•™ìŠµ ì¤‘... (30% ìœ„ì¡° ë¹„ìœ¨)\")\n    \n    # ì‹œë“œ ê³ ì •\n    seed_everything(42)\n    \n    # ëª¨ë¸ ìƒì„± (TransMIL íŠ¹ë³„ ì²˜ë¦¬)\n    if model_name == 'MeanPooling':\n        model = model_class(input_dim=256).to(device)\n    elif model_name == 'TransMIL':\n        model = model_class(\n            input_dim=256, \n            embed_dim=512, \n            num_heads=8, \n            dropout_p=0.1, \n            use_1d_ppeg=True\n        ).to(device)\n    else:\n        model = model_class(input_dim=256, hidden_dim=128, dropout_p=0.1).to(device)\n    \n    # Optimizer & Scheduler (TransMILì€ AdamW ì‚¬ìš©)\n    if model_name == 'TransMIL':\n        optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-2)\n    else:\n        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n    \n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='max', factor=0.5, patience=scheduler_patience, verbose=False\n    )\n    \n    # 30% ë°ì´í„°ë¡œ í•™ìŠµ (TransMILì€ ClassWeightedBCE ì‚¬ìš©)\n    if model_name == 'TransMIL':\n        transmil_criterion = ClassWeightedBCE(pos_weight=1.0, neg_weight=2.0)\n        inst_weight = 0.0\n    elif model_name == 'DSMIL':\n        transmil_criterion = criterion\n        inst_weight = 0.5\n    else:\n        transmil_criterion = criterion\n        inst_weight = 0.0\n    \n    model, history = train_model(\n        model, optimizer, scheduler, shift_train_loader, shift_val_loader, transmil_criterion,\n        max_epochs=max_epochs, patience=patience, name=f'{model_name}_shift',\n        inst_loss_weight=inst_weight\n    )\n    \n    # ë‹¤ì–‘í•œ ë¹„ìœ¨ë¡œ í‰ê°€\n    eval_results = {}\n    print(f\"  ğŸ“Š {model_name} ë‹¤ì–‘í•œ ë¹„ìœ¨ í‰ê°€ ì¤‘...\")\n    \n    for eval_rtag, eval_data in shift_data['eval_sets'].items():\n        eval_ratio = eval_data['ratio']\n        test_loader = DataLoader(MILDataset(eval_data['test_features'], eval_data['test']['labels']), \n                                batch_size=batch_size, shuffle=False)\n        \n        test_results = evaluate(model, test_loader, transmil_criterion)\n        eval_results[eval_rtag] = {\n            'ratio': eval_ratio,\n            'test': test_results\n        }\n        \n        print(f\"    {eval_rtag} ({eval_ratio:.0%}): AUC={test_results['auc']:.3f}, Recall={test_results['recall']:.3f}\")\n    \n    # ê²°ê³¼ ì €ì¥\n    shift_results['models'][model_name] = {\n        'model': model,\n        'history': history,\n        'evaluations': eval_results\n    }\n    \n    print(f\"  âœ… {model_name} Shift ì‹¤í—˜ ì™„ë£Œ\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"ğŸ† Shift ëª¨ë“œ ì‹¤í—˜ ì™„ë£Œ!\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "results_analysis",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T10:56:27.214631Z",
     "start_time": "2025-09-14T10:56:27.201518Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ìœ„ì¡° ë¹„ìœ¨ ì‹¤í—˜ ê²°ê³¼ ì¢…í•© ë¶„ì„\n",
      "================================================================================\n",
      "\n",
      "1ï¸âƒ£ Matched ëª¨ë“œ ê²°ê³¼ (ê° ë¹„ìœ¨ë³„ ë…ë¦½ í•™ìŠµ)\n",
      "--------------------------------------------------------------------------------\n",
      "ë¹„ìœ¨       ëª¨ë¸           AUC      F1       Precision   Recall   Accuracy  \n",
      "--------------------------------------------------------------------------------\n",
      "5%     AttentionMIL 0.626    0.000    0.000       0.000    0.500     \n",
      "5%     DSMIL        0.633    0.036    0.611       0.018    0.503     \n",
      "5%     MeanPooling  0.508    0.000    0.000       0.000    0.500     \n",
      "10%     AttentionMIL 0.644    0.003    1.000       0.002    0.501     \n",
      "10%     DSMIL        0.679    0.405    0.758       0.277    0.594     \n",
      "10%     MeanPooling  0.503    0.000    0.000       0.000    0.500     \n",
      "20%     AttentionMIL 0.779    0.586    0.766       0.475    0.665     \n",
      "20%     DSMIL        0.799    0.721    0.574       0.970    0.625     \n",
      "20%     MeanPooling  0.522    0.000    0.000       0.000    0.500     \n",
      "30%     AttentionMIL 0.804    0.745    0.708       0.787    0.731     \n",
      "30%     DSMIL        0.845    0.742    0.597       0.980    0.659     \n",
      "30%     MeanPooling  0.503    0.000    0.000       0.000    0.500     \n",
      "50%     AttentionMIL 0.834    0.695    0.808       0.610    0.733     \n",
      "50%     DSMIL        0.843    0.734    0.584       0.987    0.642     \n",
      "50%     MeanPooling  0.521    0.000    0.000       0.000    0.500     \n",
      "\n",
      "\n",
      "2ï¸âƒ£ Shift ëª¨ë“œ ê²°ê³¼ (30% í•™ìŠµ â†’ ë‹¤ì–‘í•œ ë¹„ìœ¨ í‰ê°€)\n",
      "--------------------------------------------------------------------------------\n",
      "í‰ê°€ë¹„ìœ¨     ëª¨ë¸           AUC      F1       Precision   Recall   Accuracy  \n",
      "--------------------------------------------------------------------------------\n",
      "5%       AttentionMIL 0.639    0.608    0.596       0.620    0.600     \n",
      "10%       AttentionMIL 0.648    0.614    0.599       0.628    0.604     \n",
      "20%       AttentionMIL 0.790    0.740    0.675       0.818    0.713     \n",
      "30%       AttentionMIL 0.802    0.743    0.667       0.840    0.710     \n",
      "50%       AttentionMIL 0.825    0.759    0.688       0.847    0.732     \n",
      "5%       DSMIL        0.639    0.655    0.561       0.787    0.585     \n",
      "10%       DSMIL        0.647    0.676    0.580       0.812    0.612     \n",
      "20%       DSMIL        0.790    0.731    0.603       0.928    0.659     \n",
      "30%       DSMIL        0.814    0.755    0.623       0.960    0.689     \n",
      "50%       DSMIL        0.833    0.741    0.609       0.947    0.669     \n",
      "5%       MeanPooling  0.511    0.000    0.000       0.000    0.500     \n",
      "10%       MeanPooling  0.496    0.000    0.000       0.000    0.500     \n",
      "20%       MeanPooling  0.507    0.000    0.000       0.000    0.500     \n",
      "30%       MeanPooling  0.515    0.000    0.000       0.000    0.500     \n",
      "50%       MeanPooling  0.523    0.000    0.000       0.000    0.500     \n",
      "\n",
      "âœ… ê²°ê³¼ ë¶„ì„ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# ìœ„ì¡° ë¹„ìœ¨ ì‹¤í—˜ ê²°ê³¼ ì¢…í•© ë¶„ì„\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"ğŸ“Š ìœ„ì¡° ë¹„ìœ¨ ì‹¤í—˜ ê²°ê³¼ ì¢…í•© ë¶„ì„\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Matched ëª¨ë“œ ê²°ê³¼ ìš”ì•½\n",
    "print(\"\\n1ï¸âƒ£ Matched ëª¨ë“œ ê²°ê³¼ (ê° ë¹„ìœ¨ë³„ ë…ë¦½ í•™ìŠµ)\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'ë¹„ìœ¨':<8} {'ëª¨ë¸':<12} {'AUC':<8} {'F1':<8} {'Precision':<11} {'Recall':<8} {'Accuracy':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "matched_summary = []\n",
    "for rtag, data in matched_results.items():\n",
    "    ratio = data['ratio']\n",
    "    for model_name, results in data['models'].items():\n",
    "        test = results['test']\n",
    "        print(f\"{ratio:.0%}     {model_name:<12} {test['auc']:<8.3f} {test['f1']:<8.3f} \"\n",
    "              f\"{test['precision']:<11.3f} {test['recall']:<8.3f} {test['accuracy']:<10.3f}\")\n",
    "        \n",
    "        matched_summary.append({\n",
    "            'ratio': ratio,\n",
    "            'model': model_name,\n",
    "            'auc': test['auc'],\n",
    "            'f1': test['f1'],\n",
    "            'precision': test['precision'],\n",
    "            'recall': test['recall'],\n",
    "            'accuracy': test['accuracy']\n",
    "        })\n",
    "\n",
    "# 2. Shift ëª¨ë“œ ê²°ê³¼ ìš”ì•½\n",
    "print(\"\\n\\n2ï¸âƒ£ Shift ëª¨ë“œ ê²°ê³¼ (30% í•™ìŠµ â†’ ë‹¤ì–‘í•œ ë¹„ìœ¨ í‰ê°€)\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'í‰ê°€ë¹„ìœ¨':<8} {'ëª¨ë¸':<12} {'AUC':<8} {'F1':<8} {'Precision':<11} {'Recall':<8} {'Accuracy':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "shift_summary = []\n",
    "for model_name, model_results in shift_results['models'].items():\n",
    "    for eval_rtag, eval_results in model_results['evaluations'].items():\n",
    "        ratio = eval_results['ratio']\n",
    "        test = eval_results['test']\n",
    "        print(f\"{ratio:.0%}       {model_name:<12} {test['auc']:<8.3f} {test['f1']:<8.3f} \"\n",
    "              f\"{test['precision']:<11.3f} {test['recall']:<8.3f} {test['accuracy']:<10.3f}\")\n",
    "        \n",
    "        shift_summary.append({\n",
    "            'ratio': ratio,\n",
    "            'model': model_name,\n",
    "            'auc': test['auc'],\n",
    "            'f1': test['f1'],\n",
    "            'precision': test['precision'],\n",
    "            'recall': test['recall'],\n",
    "            'accuracy': test['accuracy']\n",
    "        })\n",
    "\n",
    "# DataFrameìœ¼ë¡œ ë³€í™˜ (ë¶„ì„ í¸ì˜ì„±)\n",
    "matched_df = pd.DataFrame(matched_summary)\n",
    "shift_df = pd.DataFrame(shift_summary)\n",
    "\n",
    "print(\"\\nâœ… ê²°ê³¼ ë¶„ì„ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "performance_visualization",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:05:59.934733Z",
     "start_time": "2025-09-14T11:05:57.945277Z"
    }
   },
   "outputs": [],
   "source": "# ==============================================================================\n# ì„±ëŠ¥ ì‹œê°í™” ë° í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ë¶„ì„\n# ==============================================================================\n\n# 1. Matched ëª¨ë“œ ì„±ëŠ¥ ë³€í™” ì‹œê°í™”\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\nfig.suptitle('Matched Mode: Performance by Forgery Ratio', fontsize=16, fontweight='bold')\n\nmetrics = ['auc', 'recall', 'f1', 'precision']\nmetric_names = ['AUC', 'Recall', 'F1', 'Precision']\n\nfor i, (metric, metric_name) in enumerate(zip(metrics, metric_names)):\n    ax = axes[i//2, i%2]\n    \n    # Matched ëª¨ë“œë§Œ í”Œë¡¯ (5ê°œ ëª¨ë¸)\n    for j, model in enumerate(['AttentionMIL', 'GatedAttentionMIL', 'DSMIL', 'TransMIL', 'MeanPooling']):\n        model_data = matched_df[matched_df['model'] == model]\n        colors = ['blue', 'orange', 'red', 'purple', 'green']\n        ax.plot(model_data['ratio'], model_data[metric], 'o-', \n               color=colors[j], label=model, linewidth=2, markersize=6)\n    \n    ax.set_xlabel('Forgery Ratio')\n    ax.set_ylabel(metric_name)\n    ax.set_title(f'{metric_name} Performance (Matched)')\n    ax.grid(True, alpha=0.3)\n    ax.legend(fontsize=9)\n    \n    # xì¶• ë¼ë²¨ì„ í¼ì„¼íŠ¸ë¡œ í‘œì‹œ (50% í¬í•¨)\n    ax.set_xticks([0.05, 0.10, 0.20, 0.30, 0.50])\n    ax.set_xticklabels(['5%', '10%', '20%', '30%', '50%'])\n\nplt.tight_layout()\nplt.show()\n\n# 2. Shift ëª¨ë“œ ì„±ëŠ¥ ë³€í™” ì‹œê°í™”\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\nfig.suptitle('Shift Mode: Performance by Forgery Ratio (Trained on 30%)', fontsize=16, fontweight='bold')\n\nfor i, (metric, metric_name) in enumerate(zip(metrics, metric_names)):\n    ax = axes[i//2, i%2]\n    \n    # Shift ëª¨ë“œë§Œ í”Œë¡¯ (5ê°œ ëª¨ë¸)\n    for j, model in enumerate(['AttentionMIL', 'GatedAttentionMIL', 'DSMIL', 'TransMIL', 'MeanPooling']):\n        model_data = shift_df[shift_df['model'] == model]\n        colors = ['blue', 'orange', 'red', 'purple', 'green']\n        ax.plot(model_data['ratio'], model_data[metric], 's--', \n               color=colors[j], label=model, linewidth=2, markersize=6, alpha=0.8)\n    \n    ax.set_xlabel('Evaluation Forgery Ratio')\n    ax.set_ylabel(metric_name)\n    ax.set_title(f'{metric_name} Performance (Shift)')\n    ax.grid(True, alpha=0.3)\n    ax.legend(fontsize=9)\n    \n    # xì¶• ë¼ë²¨ì„ í¼ì„¼íŠ¸ë¡œ í‘œì‹œ (50% í¬í•¨)\n    ax.set_xticks([0.05, 0.10, 0.20, 0.30, 0.50])\n    ax.set_xticklabels(['5%', '10%', '20%', '30%', '50%'])\n\nplt.tight_layout()\nplt.show()\n\n# 3. Matched vs Shift ì§ì ‘ ë¹„êµ (AUCì™€ Recallë§Œ)\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\nfig.suptitle('Matched vs Shift Comparison', fontsize=16, fontweight='bold')\n\ncomparison_metrics = ['auc', 'recall']\ncomparison_names = ['AUC', 'Recall']\n\nfor i, (metric, metric_name) in enumerate(zip(comparison_metrics, comparison_names)):\n    ax = axes[i]\n    \n    # AttentionMIL, GatedAttentionMIL, DSMIL, TransMILë§Œ ë¹„êµ (MeanPooling ì œì™¸)\n    for j, model in enumerate(['AttentionMIL', 'GatedAttentionMIL', 'DSMIL', 'TransMIL']):\n        matched_data = matched_df[matched_df['model'] == model]\n        shift_data = shift_df[shift_df['model'] == model]\n        colors = ['blue', 'orange', 'red', 'purple']\n        \n        ax.plot(matched_data['ratio'], matched_data[metric], 'o-', \n               color=colors[j], label=f'{model} (Matched)', linewidth=2, markersize=6)\n        ax.plot(shift_data['ratio'], shift_data[metric], 's--', \n               color=colors[j], label=f'{model} (Shift)', linewidth=2, markersize=6, alpha=0.7)\n    \n    ax.set_xlabel('Forgery Ratio')\n    ax.set_ylabel(metric_name)\n    ax.set_title(f'{metric_name}: Matched vs Shift')\n    ax.grid(True, alpha=0.3)\n    ax.legend(fontsize=8)\n    \n    # xì¶• ë¼ë²¨ì„ í¼ì„¼íŠ¸ë¡œ í‘œì‹œ (50% í¬í•¨)\n    ax.set_xticks([0.05, 0.10, 0.20, 0.30, 0.50])\n    ax.set_xticklabels(['5%', '10%', '20%', '30%', '50%'])\n\nplt.tight_layout()\nplt.show()\n\n# 2. ë‚®ì€ ìœ„ì¡° ë¹„ìœ¨ì—ì„œì˜ ì„±ëŠ¥ ì €í•˜ ë¶„ì„\nprint(\"\\nğŸ” í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ë¶„ì„\")\nprint(\"=\" * 60)\n\n# 30% vs 5% ì„±ëŠ¥ ë¹„êµ (Matched ëª¨ë“œ)\nprint(\"\\nğŸ“‰ ì„±ëŠ¥ ì €í•˜ ë¶„ì„ (30% â†’ 5%, Matched ëª¨ë“œ)\")\nprint(\"-\" * 50)\nfor model in ['AttentionMIL', 'GatedAttentionMIL', 'DSMIL', 'TransMIL', 'MeanPooling']:\n    perf_30 = matched_df[(matched_df['model'] == model) & (matched_df['ratio'] == 0.30)]\n    perf_05 = matched_df[(matched_df['model'] == model) & (matched_df['ratio'] == 0.05)]\n    \n    if not perf_30.empty and not perf_05.empty:\n        auc_drop = perf_30['auc'].iloc[0] - perf_05['auc'].iloc[0]\n        recall_drop = perf_30['recall'].iloc[0] - perf_05['recall'].iloc[0]\n        \n        print(f\"{model}:\")\n        print(f\"  AUC: {perf_30['auc'].iloc[0]:.3f} â†’ {perf_05['auc'].iloc[0]:.3f} (Î”{auc_drop:+.3f})\")\n        print(f\"  Recall: {perf_30['recall'].iloc[0]:.3f} â†’ {perf_05['recall'].iloc[0]:.3f} (Î”{recall_drop:+.3f})\")\n\n# 3. Matched vs Shift ë¹„êµ (5% í‰ê°€ ê¸°ì¤€)\nprint(\"\\nğŸ†š Matched vs Shift ë¹„êµ (5% í‰ê°€ ê¸°ì¤€)\")\nprint(\"-\" * 50)\nfor model in ['AttentionMIL', 'GatedAttentionMIL', 'DSMIL', 'TransMIL', 'MeanPooling']:\n    matched_05 = matched_df[(matched_df['model'] == model) & (matched_df['ratio'] == 0.05)]\n    shift_05 = shift_df[(shift_df['model'] == model) & (shift_df['ratio'] == 0.05)]\n    \n    if not matched_05.empty and not shift_05.empty:\n        auc_diff = shift_05['auc'].iloc[0] - matched_05['auc'].iloc[0]\n        recall_diff = shift_05['recall'].iloc[0] - matched_05['recall'].iloc[0]\n        \n        print(f\"{model}:\")\n        print(f\"  AUC: Matched={matched_05['auc'].iloc[0]:.3f}, Shift={shift_05['auc'].iloc[0]:.3f} (Î”{auc_diff:+.3f})\")\n        print(f\"  Recall: Matched={matched_05['recall'].iloc[0]:.3f}, Shift={shift_05['recall'].iloc[0]:.3f} (Î”{recall_diff:+.3f})\")\n\n# 4. ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°\nprint(\"\\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸ (5% ìœ„ì¡° ë¹„ìœ¨ ê¸°ì¤€)\")\nprint(\"-\" * 50)\n\n# Matched ëª¨ë“œì—ì„œ 5% ìµœê³  ì„±ëŠ¥\nmatched_05_best = matched_df[matched_df['ratio'] == 0.05].sort_values('auc', ascending=False)\nif not matched_05_best.empty:\n    best = matched_05_best.iloc[0]\n    print(f\"Matched ëª¨ë“œ: {best['model']} (AUC: {best['auc']:.3f}, Recall: {best['recall']:.3f})\")\n\n# Shift ëª¨ë“œì—ì„œ 5% ìµœê³  ì„±ëŠ¥\nshift_05_best = shift_df[shift_df['ratio'] == 0.05].sort_values('auc', ascending=False)\nif not shift_05_best.empty:\n    best = shift_05_best.iloc[0]\n    print(f\"Shift ëª¨ë“œ: {best['model']} (AUC: {best['auc']:.3f}, Recall: {best['recall']:.3f})\")\n\nprint(\"\\nâœ… ì‹œê°í™” ë° ë¶„ì„ ì™„ë£Œ!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conclusions",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T10:56:28.001657Z",
     "start_time": "2025-09-14T10:56:27.979195Z"
    }
   },
   "outputs": [],
   "source": "# ==============================================================================\n# ê²°ë¡  ë° í•µì‹¬ ë°œê²¬ì‚¬í•­\n# ==============================================================================\n\nprint(\"ğŸ“‹ ìœ„ì¡° ë¹„ìœ¨ ì‹¤í—˜ ê²°ë¡ \")\nprint(\"=\" * 80)\n\nprint(\"\\nğŸ¯ í•µì‹¬ ì—°êµ¬ ì§ˆë¬¸ ë‹µë³€:\")\nprint(\"\\n1. ìœ„ì¡° ë¹„ìœ¨ì´ ë‚®ì•„ì§ˆìˆ˜ë¡ ì„±ëŠ¥ì´ ì–¼ë§ˆë‚˜ ì €í•˜ë˜ëŠ”ê°€?\")\n\n# í‰ê·  ì„±ëŠ¥ ì €í•˜ ê³„ì‚°\navg_drops = {}\nfor model in ['AttentionMIL', 'GatedAttentionMIL', 'DSMIL', 'TransMIL', 'MeanPooling']:\n    perf_30 = matched_df[(matched_df['model'] == model) & (matched_df['ratio'] == 0.30)]\n    perf_05 = matched_df[(matched_df['model'] == model) & (matched_df['ratio'] == 0.05)]\n    \n    if not perf_30.empty and not perf_05.empty:\n        auc_drop = (perf_30['auc'].iloc[0] - perf_05['auc'].iloc[0]) / perf_30['auc'].iloc[0] * 100\n        recall_drop = (perf_30['recall'].iloc[0] - perf_05['recall'].iloc[0]) / perf_30['recall'].iloc[0] * 100 if perf_30['recall'].iloc[0] > 0 else 0\n        avg_drops[model] = {'auc': auc_drop, 'recall': recall_drop}\n\nfor model, drops in avg_drops.items():\n    print(f\"   â€¢ {model}: AUC {drops['auc']:.1f}% ì €í•˜, Recall {drops['recall']:.1f}% ì €í•˜\")\n\nprint(\"\\n2. ì–´ë–¤ ëª¨ë¸ì´ ë‚®ì€ ìœ„ì¡° ë¹„ìœ¨ì—ì„œ ë” ê°•ê±´í•œê°€?\")\nbest_05_models = matched_df[matched_df['ratio'] == 0.05].sort_values(['recall', 'auc'], ascending=False)\nprint(f\"   â€¢ 5% ë¹„ìœ¨ì—ì„œ ìµœê³  ì„±ëŠ¥: {best_05_models.iloc[0]['model']}\")\nprint(f\"     Recall: {best_05_models.iloc[0]['recall']:.3f}, AUC: {best_05_models.iloc[0]['auc']:.3f}\")\n\nprint(\"\\n3. 30% í•™ìŠµ ëª¨ë¸ì´ 5% í…ŒìŠ¤íŠ¸ì—ì„œë„ ì˜ ë™ì‘í•˜ëŠ”ê°€? (ë„ë©”ì¸ ì ì‘)\")\ndomain_adaptation_analysis = []\nfor model in ['AttentionMIL', 'GatedAttentionMIL', 'DSMIL', 'TransMIL', 'MeanPooling']:\n    matched_05 = matched_df[(matched_df['model'] == model) & (matched_df['ratio'] == 0.05)]\n    shift_05 = shift_df[(shift_df['model'] == model) & (shift_df['ratio'] == 0.05)]\n    \n    if not matched_05.empty and not shift_05.empty:\n        performance_ratio = shift_05['auc'].iloc[0] / matched_05['auc'].iloc[0] if matched_05['auc'].iloc[0] > 0 else 0\n        domain_adaptation_analysis.append((model, performance_ratio))\n        status = \"ìš°ìˆ˜\" if performance_ratio > 0.95 else \"ë³´í†µ\" if performance_ratio > 0.85 else \"ì €ì¡°\"\n        print(f\"   â€¢ {model}: {performance_ratio:.2%} ì„±ëŠ¥ ìœ ì§€ ({status})\")\n\nprint(\"\\nğŸ“Š ì‹¤í—˜ ìš”ì•½:\")\nprint(f\"   â€¢ ì´ {len(RATIOS)}ê°œ ìœ„ì¡° ë¹„ìœ¨ í…ŒìŠ¤íŠ¸: {[f'{r:.0%}' for r in RATIOS]}\")\nprint(f\"   â€¢ {len(MODEL_CLASSES)}ê°œ ëª¨ë¸ ë¹„êµ: {list(MODEL_CLASSES.keys())}\")\nprint(f\"   â€¢ 2ê°€ì§€ ì‹¤í—˜ ëª¨ë“œ: Matched (ë…ë¦½ í•™ìŠµ) vs Shift (ë„ë©”ì¸ ì ì‘)\")\n\n# ìµœì¢… ê¶Œì¥ì‚¬í•­\nprint(\"\\nğŸ’¡ ê¶Œì¥ì‚¬í•­:\")\nbest_overall = matched_df.groupby('model')['recall'].mean().sort_values(ascending=False)\nprint(f\"   â€¢ ì „ë°˜ì  ê¶Œì¥ ëª¨ë¸: {best_overall.index[0]} (í‰ê·  Recall: {best_overall.iloc[0]:.3f})\")\nprint(f\"   â€¢ ë‚®ì€ ìœ„ì¡° ë¹„ìœ¨ ì „ìš©: {best_05_models.iloc[0]['model']}\")\n\nif any(ratio > 0.95 for _, ratio in domain_adaptation_analysis):\n    best_adapt_model = max(domain_adaptation_analysis, key=lambda x: x[1])[0]\n    print(f\"   â€¢ ë„ë©”ì¸ ì ì‘ì„± ìš°ìˆ˜: {best_adapt_model}\")\nelse:\n    print(f\"   â€¢ ë„ë©”ì¸ ì ì‘: ëª¨ë“  ëª¨ë¸ì—ì„œ ì„±ëŠ¥ ì €í•˜ ê´€ì°°, Matched ëª¨ë“œ ê¶Œì¥\")\n\nprint(\"\\nğŸ” TransMIL ì¶”ê°€ë¡œ ì–»ì€ ì¸ì‚¬ì´íŠ¸:\")\nprint(\"   â€¢ Transformer ê¸°ë°˜ ëª¨ë¸ì˜ ìœ„ì¡° ë¹„ìœ¨ ì ì‘ì„± í‰ê°€\")\nprint(\"   â€¢ Self-attention mechanismì˜ MIL ì„±ëŠ¥ ê¸°ì—¬ë„ ë¶„ì„\")\nprint(\"   â€¢ ë‹¤ì–‘í•œ ì•„í‚¤í…ì²˜(CNN, RNN, Transformer) ê°„ ì„±ëŠ¥ ë¹„êµ\")\n\nprint(\"\\nğŸ”š ìœ„ì¡° ë¹„ìœ¨ ì‹¤í—˜ ì™„ë£Œ (ì´ 5ê°œ ëª¨ë¸)\")\nprint(\"=\"*80)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}