{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39044d96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T10:53:30.948564Z",
     "start_time": "2025-09-14T10:53:23.730205Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3090\n",
      "🔒 모든 시드를 42로 고정완료\n"
     ]
    }
   ],
   "source": [
    "# 환경 설정\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, roc_curve, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# GPU 설정\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = os.getenv('MIL_STAGE3_GPU', '3')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "if device.type == 'cuda':\n",
    "    print('GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('CUDA를 사용할 수 없습니다. CPU 모드로 실행됩니다.')\n",
    "\n",
    "# 시드 고정 유틸리티 함수\n",
    "def seed_everything(seed=42):\n",
    "    \"\"\"모든 난수 생성기 시드를 고정하는 함수\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    print(f\"🔒 모든 시드를 {seed}로 고정완료\")\n",
    "\n",
    "# 초기 시드 고정\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g68jqs8g95u",
   "metadata": {},
   "source": "# Stage 3 위조 비율 실험: 다양한 위조 비율에서의 MIL 모델 성능 분석\n\n이 노트북은 Stage 2에서 생성한 **다양한 위조 비율**(5%, 10%, 20%, 30%, 50%) MIL Bag 데이터를 입력으로 받아 모델 성능을 비교 실험합니다.\n\n**실험 목표:**\n- 낮은 위조 비율(5%, 10%)에서의 모델 성능 평가\n- 위조 탐지 Recall 유지 능력 분석 (위조를 놓치지 않는 능력)\n- Matched vs Shift 모드 성능 비교\n- 모델별 위조 비율 적응성 분석\n\n**실험 모드:**\n1. **Matched 모드**: 각 비율(5/10/20/30%)별로 독립적으로 학습/평가\n2. **Shift 모드**: 30% 비율로 학습 → 다양한 비율(5/10/20/30%)로 평가\n\n**평가 모델 (총 5개):**\n- **AttentionMIL**: 기본 attention mechanism\n- **GatedAttentionMIL**: Gate로 조절되는 attention mechanism  \n- **DSMIL**: Dual-stream MIL (attention bag score + max instance score 결합)\n- **TransMIL**: Transformer 기반 MIL (1D PPEG, 표준 MultiheadAttention)\n- **MeanPooling**: 베이스라인 (단순 평균)\n\n**중요한 연구 질문:**\n- 위조 비율이 낮아질수록 성능이 얼마나 저하되는가?\n- 어떤 모델이 낮은 위조 비율에서 더 강건한가?\n- 30% 학습 모델이 5% 테스트에서도 잘 동작하는가? (도메인 적응)\n- Transformer 기반 모델(TransMIL)이 전통적인 MIL 모델보다 우수한가?"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "data_loader_utils",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T10:53:30.980747Z",
     "start_time": "2025-09-14T10:53:30.954412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 데이터 로드 유틸리티 정의 완료\n"
     ]
    }
   ],
   "source": [
    "# 위조 비율별 데이터 로드 유틸리티\n",
    "def ratio_to_tag(pos_ratio: float) -> str:\n",
    "    \"\"\"비율을 파일명 태그로 변환: 0.05 → '05p', 0.30 → '30p'\"\"\"\n",
    "    return f\"{int(round(pos_ratio*100)):02d}p\"\n",
    "\n",
    "def load_forgery_data(bags_dir, margin='0.4', ratios=[0.05, 0.10, 0.20, 0.30, 0.50]):\n",
    "    \"\"\"다양한 위조 비율의 MIL Bag 데이터를 로드\"\"\"\n",
    "    data_dict = {}\n",
    "    \n",
    "    print(\"📁 위조 비율별 데이터 로딩 중...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for ratio in ratios:\n",
    "        rtag = ratio_to_tag(ratio)\n",
    "        print(f\"  Loading {rtag} ({ratio:.0%}) datasets...\")\n",
    "        \n",
    "        # 파일 경로 생성\n",
    "        train_pkl = os.path.join(bags_dir, f'bags_arcface_margin_{margin}_{rtag}_random_train.pkl')\n",
    "        val_pkl = os.path.join(bags_dir, f'bags_arcface_margin_{margin}_{rtag}_random_val.pkl')\n",
    "        test_pkl = os.path.join(bags_dir, f'bags_arcface_margin_{margin}_{rtag}_random_test.pkl')\n",
    "        \n",
    "        # 데이터 로드\n",
    "        try:\n",
    "            with open(train_pkl, 'rb') as f:\n",
    "                train_data = pickle.load(f)\n",
    "            with open(val_pkl, 'rb') as f:\n",
    "                val_data = pickle.load(f)\n",
    "            with open(test_pkl, 'rb') as f:\n",
    "                test_data = pickle.load(f)\n",
    "                \n",
    "            data_dict[rtag] = {\n",
    "                'train': train_data,\n",
    "                'val': val_data,\n",
    "                'test': test_data,\n",
    "                'ratio': ratio\n",
    "            }\n",
    "            \n",
    "            print(f\"    ✅ {rtag}: Train={len(train_data['labels'])}, Val={len(val_data['labels'])}, Test={len(test_data['labels'])}\")\n",
    "            \n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"    ❌ {rtag}: 파일을 찾을 수 없습니다 - {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"✅ 총 {len(data_dict)}개 비율 데이터셋 로드 완료\")\n",
    "    return data_dict\n",
    "\n",
    "def load_shift_data(bags_dir, margin='0.4', train_ratio=0.30, eval_ratios=[0.05, 0.10, 0.20, 0.30, 0.50]):\n",
    "    \"\"\"Shift 모드용 데이터 로드: 30% 학습 + 다양한 비율 평가\"\"\"\n",
    "    print(\"📁 Shift 모드 데이터 로딩 중...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 학습용 데이터 (30%)\n",
    "    train_rtag = ratio_to_tag(train_ratio)\n",
    "    train_pkl = os.path.join(bags_dir, f'bags_arcface_margin_{margin}_{train_rtag}_random_train.pkl')\n",
    "    val_pkl = os.path.join(bags_dir, f'bags_arcface_margin_{margin}_{train_rtag}_random_val.pkl')\n",
    "    # Shift 전용 네임스페이스 파일이 있으면 우선 사용\n",
    "    train_pkl_shift = os.path.join(bags_dir, f'bags_arcface_margin_{margin}_{train_rtag}_random_train_shiftbase.pkl')\n",
    "    val_pkl_shift   = os.path.join(bags_dir, f'bags_arcface_margin_{margin}_{train_rtag}_random_val_shiftbase.pkl')\n",
    "    if os.path.exists(train_pkl_shift):\n",
    "        train_pkl = train_pkl_shift\n",
    "    if os.path.exists(val_pkl_shift):\n",
    "        val_pkl = val_pkl_shift\n",
    "\n",
    "\n",
    "    \n",
    "    with open(train_pkl, 'rb') as f:\n",
    "        train_data = pickle.load(f)\n",
    "    with open(val_pkl, 'rb') as f:\n",
    "        val_data = pickle.load(f)\n",
    "    \n",
    "    print(f\"  학습용 {train_rtag} ({train_ratio:.0%}): Train={len(train_data['labels'])}, Val={len(val_data['labels'])}\")\n",
    "    \n",
    "    # 평가용 데이터들\n",
    "    eval_data = {}\n",
    "    for ratio in eval_ratios:\n",
    "        rtag = ratio_to_tag(ratio)\n",
    "        test_pkl = os.path.join(bags_dir, f'bags_arcface_margin_{margin}_{rtag}_random_test_shift_{rtag}.pkl')\n",
    "        \n",
    "        # Shift 평가 파일이 없으면 일반 테스트 파일 사용\n",
    "        if not os.path.exists(test_pkl):\n",
    "            test_pkl = os.path.join(bags_dir, f'bags_arcface_margin_{margin}_{rtag}_random_test.pkl')\n",
    "        \n",
    "        try:\n",
    "            with open(test_pkl, 'rb') as f:\n",
    "                test_data = pickle.load(f)\n",
    "            eval_data[rtag] = {\n",
    "                'test': test_data,\n",
    "                'ratio': ratio\n",
    "            }\n",
    "            print(f\"  평가용 {rtag} ({ratio:.0%}): Test={len(test_data['labels'])}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"  ❌ {rtag}: 평가 파일을 찾을 수 없습니다\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"✅ Shift 모드 데이터 로드 완료: 학습 {train_rtag} + 평가 {len(eval_data)}개\")\n",
    "    \n",
    "    return {\n",
    "        'train': train_data,\n",
    "        'val': val_data,\n",
    "        'eval_sets': eval_data,\n",
    "        'train_ratio': train_ratio\n",
    "    }\n",
    "\n",
    "print(\"✅ 데이터 로드 유틸리티 정의 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4742eb10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T10:53:36.819752Z",
     "start_time": "2025-09-14T10:53:30.982857Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 위조 비율별 데이터 로딩 중...\n",
      "============================================================\n",
      "  Loading 05p (5%) datasets...\n",
      "    ✅ 05p: Train=3600, Val=1200, Test=1200\n",
      "  Loading 10p (10%) datasets...\n",
      "    ✅ 10p: Train=3600, Val=1200, Test=1200\n",
      "  Loading 20p (20%) datasets...\n",
      "    ✅ 20p: Train=3600, Val=1200, Test=1200\n",
      "  Loading 30p (30%) datasets...\n",
      "    ✅ 30p: Train=3600, Val=1200, Test=1200\n",
      "  Loading 50p (50%) datasets...\n",
      "    ✅ 50p: Train=3600, Val=1200, Test=1200\n",
      "============================================================\n",
      "✅ 총 5개 비율 데이터셋 로드 완료\n",
      "📁 Shift 모드 데이터 로딩 중...\n",
      "============================================================\n",
      "  학습용 30p (30%): Train=3600, Val=1200\n",
      "  평가용 05p (5%): Test=1200\n",
      "  평가용 10p (10%): Test=1200\n",
      "  평가용 20p (20%): Test=1200\n",
      "  평가용 30p (30%): Test=1200\n",
      "  평가용 50p (50%): Test=1200\n",
      "============================================================\n",
      "✅ Shift 모드 데이터 로드 완료: 학습 30p + 평가 5개\n",
      "\n",
      "🔄 Instance mean 계산 중...\n",
      "✅ 모든 데이터 전처리 완료\n"
     ]
    }
   ],
   "source": [
    "# 위조 비율별 데이터 로드\n",
    "embedding_margin = '0.4'\n",
    "bags_dir = '/workspace/MIL/data/processed/bags'\n",
    "RATIOS = [0.05, 0.10, 0.20, 0.30, 0.50]\n",
    "\n",
    "# Matched 모드용 데이터 로드\n",
    "matched_data = load_forgery_data(bags_dir, embedding_margin, RATIOS)\n",
    "\n",
    "# Shift 모드용 데이터 로드\n",
    "shift_data = load_shift_data(bags_dir, embedding_margin, train_ratio=0.30, eval_ratios=RATIOS)\n",
    "\n",
    "# Instance mean 계산 함수: (10,5,256) → (10,256)\n",
    "def to_instance_means(bags):\n",
    "    return [bag.mean(axis=1).astype(np.float32) for bag in bags]\n",
    "\n",
    "print(\"\\n🔄 Instance mean 계산 중...\")\n",
    "# Matched 데이터 전처리\n",
    "for rtag, data in matched_data.items():\n",
    "    data['train_features'] = to_instance_means(data['train']['bags'])\n",
    "    data['val_features'] = to_instance_means(data['val']['bags'])\n",
    "    data['test_features'] = to_instance_means(data['test']['bags'])\n",
    "    \n",
    "# Shift 데이터 전처리\n",
    "shift_data['train_features'] = to_instance_means(shift_data['train']['bags'])\n",
    "shift_data['val_features'] = to_instance_means(shift_data['val']['bags'])\n",
    "for rtag, eval_set in shift_data['eval_sets'].items():\n",
    "    eval_set['test_features'] = to_instance_means(eval_set['test']['bags'])\n",
    "\n",
    "print(\"✅ 모든 데이터 전처리 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17dbf94a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T10:53:36.831563Z",
     "start_time": "2025-09-14T10:53:36.823803Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset 클래스 및 DataLoader 유틸리티 정의 완료\n"
     ]
    }
   ],
   "source": [
    "# Dataset 클래스 (on‑the‑fly Tensor 변환)\n",
    "class MILDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features  # list of np.ndarray\n",
    "        self.labels = labels      # list of int\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return x, y\n",
    "\n",
    "def create_dataloaders(features, labels, batch_size=16):\n",
    "    \"\"\"Features와 labels로부터 DataLoader 생성\"\"\"\n",
    "    train_loader = DataLoader(MILDataset(features['train'], labels['train']), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(MILDataset(features['val'], labels['val']), batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(MILDataset(features['test'], labels['test']), batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "print(\"✅ Dataset 클래스 및 DataLoader 유틸리티 정의 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "qpxriysblvi",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T10:53:36.840044Z",
     "start_time": "2025-09-14T10:53:36.833780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ WeightedBCE 손실함수 클래스 정의 완료\n",
      "   FP Weight: 2.0 (baseline과 동일)\n"
     ]
    }
   ],
   "source": [
    "# WeightedBCE 손실함수 정의\n",
    "class WeightedBCE(nn.Module):\n",
    "    \"\"\"Weighted Binary Cross Entropy Loss - False Positive에 더 큰 가중치 부여\"\"\"\n",
    "    def __init__(self, fp_weight=2.0):\n",
    "        super().__init__()\n",
    "        self.fp_weight = fp_weight\n",
    "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
    "    \n",
    "    def forward(self, logits, labels):\n",
    "        loss = self.bce(logits, labels)\n",
    "        # False Positive (label=0인데 예측이 1)에 더 큰 가중치\n",
    "        fp_mask = (labels == 0).float()\n",
    "        loss = loss * (1 + self.fp_weight * fp_mask)\n",
    "        return loss.mean()\n",
    "\n",
    "print(\"✅ WeightedBCE 손실함수 클래스 정의 완료\")\n",
    "print(f\"   FP Weight: 2.0 (baseline과 동일)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79407013",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T10:53:36.864074Z",
     "start_time": "2025-09-14T10:53:36.842802Z"
    }
   },
   "outputs": [],
   "source": "# MIL 모델 정의: Attention vs DSMIL vs Gated vs Mean Pooling vs TransMIL\n\nclass AttentionMIL(nn.Module):\n    \"\"\"기본 Attention-based MIL 모델\"\"\"\n    def __init__(self, input_dim=256, hidden_dim=128, dropout_p=0.1):\n        super().__init__()\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        \n        # Instance-level feature transformation\n        self.instance_fc = nn.Linear(input_dim, hidden_dim)\n        self.dropout = nn.Dropout(dropout_p)\n        \n        # Attention mechanism\n        self.att_fc1 = nn.Linear(hidden_dim, hidden_dim)\n        self.att_fc2 = nn.Linear(hidden_dim, 1)\n        \n        # Classifier\n        self.classifier = nn.Linear(hidden_dim, 1)\n        self._init_weights()\n    \n    def _init_weights(self):\n        # He initialization for ReLU layers\n        nn.init.kaiming_uniform_(self.instance_fc.weight, nonlinearity='relu')\n        nn.init.zeros_(self.instance_fc.bias)\n        # Xavier for others\n        nn.init.xavier_uniform_(self.att_fc1.weight)\n        nn.init.zeros_(self.att_fc1.bias)\n        nn.init.xavier_uniform_(self.att_fc2.weight)\n        nn.init.zeros_(self.att_fc2.bias)\n        nn.init.xavier_uniform_(self.classifier.weight)\n        nn.init.zeros_(self.classifier.bias)\n    \n    def forward(self, x):\n        # Instance feature transformation: (B, N, input_dim) -> (B, N, hidden_dim)\n        h = torch.relu(self.instance_fc(x))\n        h = self.dropout(h)\n        \n        # Attention calculation\n        a = torch.tanh(self.att_fc1(h))  # (B, N, hidden_dim)\n        a = self.att_fc2(a).squeeze(-1)  # (B, N)\n        weights = torch.softmax(a, dim=1)  # (B, N)\n        \n        # Weighted aggregation\n        bag_repr = torch.sum(weights.unsqueeze(-1) * h, dim=1)  # (B, hidden_dim)\n        bag_repr = self.dropout(bag_repr)\n        \n        # Classification\n        logits = self.classifier(bag_repr).squeeze(-1)  # (B,)\n        return logits, weights\n\nclass GatedAttentionMIL(nn.Module):\n    \"\"\"Gated Attention MIL 모델 - Gate mechanism으로 attention 조절\"\"\"\n    def __init__(self, input_dim=256, hidden_dim=128, dropout_p=0.1):\n        super().__init__()\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        \n        # Instance-level feature transformation\n        self.instance_fc = nn.Linear(input_dim, hidden_dim)\n        self.dropout = nn.Dropout(dropout_p)\n        \n        # Attention branch\n        self.attention_fc1 = nn.Linear(hidden_dim, hidden_dim)\n        self.attention_fc2 = nn.Linear(hidden_dim, 1)\n        \n        # Gate branch  \n        self.gate_fc1 = nn.Linear(hidden_dim, hidden_dim)\n        \n        # Classifier\n        self.classifier = nn.Linear(hidden_dim, 1)\n        self._init_weights()\n    \n    def _init_weights(self):\n        # He initialization for ReLU layers\n        nn.init.kaiming_uniform_(self.instance_fc.weight, nonlinearity='relu')\n        nn.init.zeros_(self.instance_fc.bias)\n        \n        # Xavier for attention branch\n        nn.init.xavier_uniform_(self.attention_fc1.weight)\n        nn.init.zeros_(self.attention_fc1.bias)\n        nn.init.xavier_uniform_(self.attention_fc2.weight)\n        nn.init.zeros_(self.attention_fc2.bias)\n        \n        # Xavier for gate branch\n        nn.init.xavier_uniform_(self.gate_fc1.weight)\n        nn.init.zeros_(self.gate_fc1.bias)\n        \n        # Classifier\n        nn.init.xavier_uniform_(self.classifier.weight)\n        nn.init.zeros_(self.classifier.bias)\n    \n    def forward(self, x):\n        # Instance feature transformation: (B, N, input_dim) -> (B, N, hidden_dim)\n        h = torch.relu(self.instance_fc(x))\n        h = self.dropout(h)\n        \n        # Attention branch\n        attention = torch.tanh(self.attention_fc1(h))  # (B, N, hidden_dim)\n        \n        # Gate branch (vector gate)\n        gate = torch.sigmoid(self.gate_fc1(h))  # (B, N, hidden_dim) - vector gate\n        \n        # Combine attention with gate (canonical: vector-level gating)\n        scores = self.attention_fc2(attention * gate).squeeze(-1)  # (B, N)\n        \n        # Softmax normalization\n        weights = torch.softmax(scores, dim=1)  # (B, N)\n        \n        # Weighted aggregation\n        bag_repr = torch.sum(weights.unsqueeze(-1) * h, dim=1)  # (B, hidden_dim)\n        bag_repr = self.dropout(bag_repr)\n        \n        # Classification\n        logits = self.classifier(bag_repr).squeeze(-1)  # (B,)\n        return logits, weights\n\nclass DSMILModel(nn.Module):\n    \"\"\"DSMIL (원 논문 정합 버전)\n    - instance branch: inst_logits (B, N)\n    - aggregator branch: critical instance 기반 cross-attention으로 bag_logit_attn (B,)\n    - final: bag_logits = alpha * bag_logit_attn + (1 - alpha) * max_inst_logit\n    반환: bag_logits, attn_w, inst_logits, top_idx\n    \"\"\"\n    def __init__(self, input_dim=256, hidden_dim=128, dropout_p=0.1, alpha=0.5, temperature=None):\n        super().__init__()\n        self.enc = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout_p)\n        )\n        # instance classifier (instance branch)\n        self.inst_cls = nn.Linear(hidden_dim, 1)\n\n        # DSMIL aggregator: single projection for queries + values\n        self.query_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.val_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.bag_cls = nn.Linear(hidden_dim, 1)\n\n        # dual-stream 결합 파라미터\n        self.alpha = alpha\n        self.temperature = temperature\n\n        # weight init\n        nn.init.xavier_uniform_(self.inst_cls.weight); nn.init.zeros_(self.inst_cls.bias)\n        nn.init.xavier_uniform_(self.query_proj.weight)\n        nn.init.xavier_uniform_(self.val_proj.weight)\n        nn.init.xavier_uniform_(self.bag_cls.weight); nn.init.zeros_(self.bag_cls.bias)\n\n    def forward(self, x):\n        # x: (B, N, D)\n        h = self.enc(x)                                # (B, N, H)\n        inst_logits = self.inst_cls(h).squeeze(-1)     # (B, N)\n\n        # critical instance (top-1 by logit)\n        top_idx = inst_logits.argmax(dim=1)            # (B,)\n        B, N, H = h.shape\n        batch = torch.arange(B, device=h.device)\n\n        # projector outputs\n        Q = self.query_proj(h)                         # (B, N, H)\n        q_star = Q[batch, top_idx]                     # (B, H)\n        attn_score = torch.bmm(Q, q_star.unsqueeze(-1)).squeeze(-1)  # (B, N)\n\n        scale = (H ** 0.5) if self.temperature is None else self.temperature\n        attn_w = torch.softmax(attn_score / scale, dim=1)           # (B, N)\n\n        V = self.val_proj(h)                           # (B, N, H)\n        attn_repr = (attn_w.unsqueeze(-1) * V).sum(dim=1)           # (B, H)\n        bag_logit_attn = self.bag_cls(attn_repr).squeeze(-1)        # (B,)\n\n        # dual-stream 결합: instance-score max\n        max_inst_logit = inst_logits.max(dim=1).values              # (B,)\n        bag_logits = self.alpha * bag_logit_attn + (1.0 - self.alpha) * max_inst_logit\n\n        return bag_logits, attn_w, inst_logits, top_idx\n\n\n# TransMIL 모델 구성요소들 (stage3_baseline_transmil.ipynb에서 정확히 복사)\n\nclass TransBlock(nn.Module):\n    \"\"\"표준 Transformer 블록 (FFN 포함)\"\"\"\n    def __init__(self, dim=512, num_heads=8, dropout_p=0.1, ffn_mult=4):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(dim)\n        self.attn = nn.MultiheadAttention(\n            embed_dim=dim, num_heads=num_heads, dropout=dropout_p, batch_first=True\n        )\n        self.norm2 = nn.LayerNorm(dim)\n        self.ffn = nn.Sequential(\n            nn.Linear(dim, ffn_mult * dim),\n            nn.GELU(),  # ViT 계열은 GELU가 보편적\n            nn.Dropout(dropout_p),\n            nn.Linear(ffn_mult * dim, dim),\n            nn.Dropout(dropout_p),\n        )\n    \n    def forward(self, x):\n        # Pre-norm style\n        normed_x = self.norm1(x)\n        attn_out, _ = self.attn(normed_x, normed_x, normed_x, need_weights=False)\n        x = x + attn_out\n        \n        ffn_out = self.ffn(self.norm2(x))\n        x = x + ffn_out\n        return x\n\n\nclass PPEG1D(nn.Module):\n    \"\"\"1D PPEG (depthwise Conv1d)\"\"\"\n    def __init__(self, dim=512):\n        super().__init__()\n        self.proj7 = nn.Conv1d(dim, dim, kernel_size=7, padding=3, groups=dim)\n        self.proj5 = nn.Conv1d(dim, dim, kernel_size=5, padding=2, groups=dim)\n        self.proj3 = nn.Conv1d(dim, dim, kernel_size=3, padding=1, groups=dim)\n\n    def forward(self, x):\n        # x: (B, 1+N, C) = [CLS | tokens]\n        cls_token, feat_token = x[:, :1], x[:, 1:]  # (B,1,C), (B,N,C)\n        b, n, c = feat_token.shape\n        feat = feat_token.transpose(1, 2)  # (B,C,N)\n        feat = self.proj7(feat) + self.proj5(feat) + self.proj3(feat) + feat\n        feat = feat.transpose(1, 2)  # (B,N,C)\n        return torch.cat([cls_token, feat], dim=1)  # (B,1+N,C)\n\n\nclass LearnablePosEmb1D(nn.Module):\n    \"\"\"간단한 학습형 1D 위치임베딩\"\"\"\n    def __init__(self, max_len=512, dim=512):\n        super().__init__()\n        self.pos = nn.Parameter(torch.zeros(1, max_len, dim))\n        nn.init.trunc_normal_(self.pos, std=0.02)\n    \n    def forward(self, x):\n        # x: (B, 1+N, C)\n        return x + self.pos[:, :x.size(1), :]\n\n\nclass TransMIL(nn.Module):\n    \"\"\"수정된 TransMIL - 1D 시퀀스용으로 재설계\"\"\"\n    def __init__(self, input_dim=256, embed_dim=512, num_heads=8, dropout_p=0.1, n_classes=1,\n                 use_1d_ppeg=True, max_len=512):\n        super().__init__()\n        assert embed_dim % num_heads == 0\n\n        self.embed = nn.Sequential(\n            nn.Linear(input_dim, embed_dim),\n            nn.GELU(),  # ReLU → GELU 권장\n            nn.Dropout(dropout_p),\n        )\n        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim) * 0.02)\n\n        # 두 개의 Transformer 블록\n        self.block1 = TransBlock(embed_dim, num_heads, dropout_p)\n        self.block2 = TransBlock(embed_dim, num_heads, dropout_p)\n\n        # 위치 부여: 1D PPEG 또는 learnable pos emb\n        if use_1d_ppeg:\n            self.pos_layer = PPEG1D(embed_dim)\n            self.use_learnable_pos = False\n        else:\n            self.pos_layer = LearnablePosEmb1D(max_len=max_len, dim=embed_dim)\n            self.use_learnable_pos = True\n\n        self.norm = nn.LayerNorm(embed_dim)\n        self.classifier = nn.Linear(embed_dim, n_classes)\n        self._init_weights()\n\n    def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, (nn.Conv1d, nn.Conv2d)):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        \"\"\"\n        x: (B, N, D_in)  # N = 인스턴스 수 (정사각 패딩 없음)\n        \"\"\"\n        assert x.dim() == 3, 'Input must be (batch, instances, features).'\n        h = self.embed(x)  # (B,N,C)\n        b, n, c = h.shape\n        cls = self.cls_token.expand(b, 1, c)  # (B,1,C)\n        h = torch.cat([cls, h], dim=1)  # (B,1+N,C)\n\n        # 위치부여\n        if self.use_learnable_pos:\n            h = self.pos_layer(h)  # learnable pos emb\n            h = self.block1(h)\n            h = self.block2(h)\n        else:\n            h = self.block1(h)  # Pre-attn로 약간 섞은 뒤\n            h = self.pos_layer(h)  # 1D PPEG\n            h = self.block2(h)\n\n        h = self.norm(h)\n        cls_out = h[:, 0]  # (B,C)\n        logits = self.classifier(cls_out)  # (B,1)\n        return logits.squeeze(-1)\n\n\nclass ClassWeightedBCE(nn.Module):\n    \"\"\"명확한 클래스 가중 BCE\"\"\"\n    def __init__(self, pos_weight=1.0, neg_weight=1.0):\n        super().__init__()\n        self.pos_weight = pos_weight\n        self.neg_weight = neg_weight\n    \n    def forward(self, logits, labels):\n        # labels: {0,1} float tensor\n        weights = labels * self.pos_weight + (1.0 - labels) * self.neg_weight\n        return nn.functional.binary_cross_entropy_with_logits(\n            logits, labels, weight=weights, reduction='mean'\n        )\n\n\nclass MeanPoolingModel(nn.Module):\n    \"\"\"베이스라인: 단순 평균 풀링 모델\"\"\"\n    def __init__(self, input_dim=256):\n        super().__init__()\n        self.fc = nn.Linear(input_dim, 1)\n        nn.init.xavier_uniform_(self.fc.weight)\n        nn.init.zeros_(self.fc.bias)\n    \n    def forward(self, x):\n        bag_mean = x.mean(dim=1)  # (B, input_dim)\n        logits = self.fc(bag_mean).squeeze(-1)  # (B,)\n        return logits\n\nprint(\"✅ 모델 클래스 정의 완료:\")\nprint(\"  - AttentionMIL: 기본 attention mechanism\")\nprint(\"  - GatedAttentionMIL: Gate로 조절되는 attention mechanism\")\nprint(\"  - DSMILModel: Dual-stream MIL (attention bag score + max instance score 평균)\")\nprint(\"  - TransMIL: Transformer 기반 MIL (1D PPEG, 표준 MultiheadAttention)\")\nprint(\"  - MeanPoolingModel: 베이스라인 (단순 평균)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bb9c1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T10:53:36.883930Z",
     "start_time": "2025-09-14T10:53:36.866378Z"
    }
   },
   "outputs": [],
   "source": "# 모델 학습 및 평가 함수\nfrom sklearn.metrics import precision_score, recall_score\n\ndef is_attention_model(model):\n    \"\"\"Attention 기반 모델인지 확인 (AttentionMIL, GatedAttentionMIL, DSMILModel)\"\"\"\n    return isinstance(model, (AttentionMIL, GatedAttentionMIL, DSMILModel))\n\ndef _extract_logits(model, X):\n    \"\"\"모델 출력에서 logits 추출 (TransMIL 지원)\"\"\"\n    output = model(X)\n    if isinstance(output, dict):\n        if 'logits' not in output:\n            raise KeyError('Model output dictionary must contain a `logits` key.')\n        logits = output['logits']\n    elif isinstance(output, tuple):\n        logits = output[0]\n    else:\n        logits = output\n    if isinstance(logits, (list, tuple)):\n        logits = logits[0]\n    if hasattr(logits, 'dim') and logits.dim() == 2 and logits.size(-1) == 1:\n        logits = logits.squeeze(-1)\n    return logits\n\ndef train_one_epoch(model, optimizer, loader, bag_criterion, inst_loss_weight=0.5):\n    model.train()\n    total_loss, preds_all, labels_all = 0.0, [], []\n    inst_criterion = nn.BCEWithLogitsLoss()\n    for X, y in tqdm(loader, desc='Train', leave=False):\n        X, y = X.to(device), y.to(device)\n        optimizer.zero_grad()\n        out = model(X)\n        \n        # TransMIL 모델의 경우 단일 logits 반환\n        if isinstance(model, TransMIL):\n            logits = out\n            loss = bag_criterion(logits, y)\n        elif isinstance(out, (tuple, list)) and len(out) == 4:\n            logits, _, inst_logits, top_idx = out\n            bag_loss = bag_criterion(logits, y)\n            if inst_loss_weight > 0:\n                batch_idx = torch.arange(y.size(0), device=y.device)\n                top_inst_logits = inst_logits[batch_idx, top_idx]\n                inst_loss = inst_criterion(top_inst_logits, y)\n                loss = bag_loss + inst_loss_weight * inst_loss\n            else:\n                loss = bag_loss\n        else:\n            logits = out[0] if isinstance(out, (tuple, list)) else out\n            loss = bag_criterion(logits, y)\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        total_loss += loss.item() * y.size(0)\n        preds_all.extend((torch.sigmoid(logits) >= 0.5).float().cpu().numpy())\n        labels_all.extend(y.cpu().numpy())\n    return total_loss / len(loader.dataset), accuracy_score(labels_all, preds_all)\n\ndef evaluate(model, loader, criterion):\n    model.eval()\n    total_loss = 0.0\n    probs_all, preds_all, labels_all = [], [], []\n    attention_weights_all = []\n\n    with torch.no_grad():\n        for X, y in tqdm(loader, desc='Eval', leave=False):\n            X, y = X.to(device), y.to(device)\n            out = model(X)\n            \n            # TransMIL 모델의 경우 단일 logits 반환\n            if isinstance(model, TransMIL):\n                logits = out\n            elif isinstance(out, (tuple, list)):\n                logits = out[0]\n                if len(out) >= 2 and out[1] is not None:\n                    attention_weights_all.append(out[1].detach().cpu().numpy())\n            else:\n                logits = out\n                \n            loss = criterion(logits, y)\n            total_loss += loss.item() * y.size(0)\n            probs = torch.sigmoid(logits)\n            preds = (probs >= 0.5).float()\n            probs_all.extend(probs.cpu().numpy())\n            preds_all.extend(preds.cpu().numpy())\n            labels_all.extend(y.cpu().numpy())\n\n    acc = accuracy_score(labels_all, preds_all)\n    auc = roc_auc_score(labels_all, probs_all) if len(set(labels_all)) > 1 else 0.0\n    f1 = f1_score(labels_all, preds_all) if len(set(preds_all)) > 1 else 0.0\n    precision = precision_score(labels_all, preds_all, zero_division=0.0)\n    recall = recall_score(labels_all, preds_all, zero_division=0.0)\n\n    attention_weights_combined = np.concatenate(attention_weights_all, axis=0) if attention_weights_all else None\n\n    return {\n        'loss': total_loss / len(loader.dataset),\n        'accuracy': acc,\n        'auc': auc,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall,\n        'probs': np.array(probs_all),\n        'labels': np.array(labels_all),\n        'preds': np.array(preds_all),\n        'attention_weights': attention_weights_combined\n    }\n\ndef train_model(model, optimizer, scheduler, train_loader, val_loader, criterion, \n                max_epochs=10, patience=3, name='model', inst_loss_weight=0.0):\n    \"\"\"모델 학습 (Early Stopping 포함)\"\"\"\n    best_auc = 0.0\n    best_state = None\n    epochs_no_improve = 0\n    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'val_auc': [], 'val_f1': []}\n\n    print(f\"\\n🚀 {name} 모델 학습 시작...\")\n    print(f\"   Max epochs: {max_epochs}, Patience: {patience}\")\n\n    for epoch in range(1, max_epochs + 1):\n        print(f\"\\nEpoch {epoch}/{max_epochs} – {name}\")\n        tr_loss, tr_acc = train_one_epoch(model, optimizer, train_loader, criterion, inst_loss_weight=inst_loss_weight)\n        val_results = evaluate(model, val_loader, criterion)\n        val_loss, val_acc, val_auc, val_f1 = val_results['loss'], val_results['accuracy'], val_results['auc'], val_results['f1']\n\n        history['train_loss'].append(tr_loss)\n        history['train_acc'].append(tr_acc)\n        history['val_loss'].append(val_loss)\n        history['val_acc'].append(val_acc)\n        history['val_auc'].append(val_auc)\n        history['val_f1'].append(val_f1)\n\n        print(f\"  Train: Loss={tr_loss:.4f}, Acc={tr_acc:.4f}\")\n        print(f\"  Val:   Loss={val_loss:.4f}, Acc={val_acc:.4f}, AUC={val_auc:.4f}, F1={val_f1:.4f}\")\n\n        scheduler.step(val_auc)\n\n        if val_auc > best_auc:\n            best_auc = val_auc\n            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n            torch.save(best_state, f'best_{name}.pth')\n            print(f\"  ✅ New best AUC: {best_auc:.4f} – model saved.\")\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n            print(f\"  ⏳ No improvement. Patience: {epochs_no_improve}/{patience}\")\n            if epochs_no_improve >= patience:\n                print(\"  🛑 Early stopping triggered.\")\n                break\n\n    if best_state is not None:\n        model.load_state_dict(best_state)\n        print(f\"  📂 Best model loaded (AUC: {best_auc:.4f})\")\n\n    return model, history\n\nprint(\"✅ 학습/평가 함수 정의 완료:\")\nprint(\"  - 다중 모델 지원 (Attention/Gated/DSMIL/TransMIL/MeanPooling)\")\nprint(\"  - TransMIL 단일 logits 출력 지원\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched_experiments",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T10:56:02.092591Z",
     "start_time": "2025-09-14T10:53:36.886201Z"
    }
   },
   "outputs": [],
   "source": "# ==============================================================================\n# Matched 모드 실험: 각 비율별로 독립적으로 학습/평가\n# ==============================================================================\n\nprint(\"🔬 Matched 모드 실험 시작\")\nprint(\"=\" * 80)\nprint(\"각 위조 비율별로 독립적으로 모델을 학습하고 평가합니다.\")\nprint(\"목표: 위조 비율 변화에 따른 모델 성능 변화 분석\")\nprint(\"=\" * 80)\n\n# 실험 설정\ncriterion = WeightedBCE(fp_weight=2.0)\nlearning_rate = 1e-3\nmax_epochs = 10\npatience = 3\nscheduler_patience = 1\nbatch_size = 16\n\n# 모델 리스트 (TransMIL 추가 - 총 5개 모델)\nMODEL_CLASSES = {\n    'AttentionMIL': AttentionMIL,\n    'GatedAttentionMIL': GatedAttentionMIL,\n    'DSMIL': DSMILModel,\n    'TransMIL': TransMIL,\n    'MeanPooling': MeanPoolingModel\n}\n\n# 결과 저장\nmatched_results = {}\n\n# 각 비율별로 실험\nfor rtag, data in matched_data.items():\n    ratio = data['ratio']\n    print(f\"\\n{'='*60}\")\n    print(f\"🎯 {rtag} ({ratio:.0%}) 위조 비율 실험\")\n    print(f\"{'='*60}\")\n    \n    # DataLoader 생성\n    features = {\n        'train': data['train_features'],\n        'val': data['val_features'],\n        'test': data['test_features']\n    }\n    labels = {\n        'train': data['train']['labels'],\n        'val': data['val']['labels'],\n        'test': data['test']['labels']\n    }\n    \n    train_loader, val_loader, test_loader = create_dataloaders(features, labels, batch_size)\n    \n    matched_results[rtag] = {'ratio': ratio, 'models': {}}\n    \n    # 각 모델별로 학습\n    for model_name, model_class in MODEL_CLASSES.items():\n        print(f\"\\n🚀 {model_name} 학습 중... (비율: {rtag})\")\n        \n        # 시드 고정 (재현성)\n        seed_everything(42)\n        \n        # 모델 생성 (TransMIL 특별 처리)\n        if model_name == 'MeanPooling':\n            model = model_class(input_dim=256).to(device)\n        elif model_name == 'TransMIL':\n            model = model_class(\n                input_dim=256, \n                embed_dim=512, \n                num_heads=8, \n                dropout_p=0.1, \n                use_1d_ppeg=True\n            ).to(device)\n        else:\n            model = model_class(input_dim=256, hidden_dim=128, dropout_p=0.1).to(device)\n        \n        # Optimizer & Scheduler (TransMIL은 AdamW 사용)\n        if model_name == 'TransMIL':\n            optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-2)\n        else:\n            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n        \n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, mode='max', factor=0.5, patience=scheduler_patience, verbose=False\n        )\n        \n        # 학습 (TransMIL은 ClassWeightedBCE 사용)\n        if model_name == 'TransMIL':\n            transmil_criterion = ClassWeightedBCE(pos_weight=1.0, neg_weight=2.0)\n            inst_weight = 0.0\n        elif model_name == 'DSMIL':\n            transmil_criterion = criterion\n            inst_weight = 0.5\n        else:\n            transmil_criterion = criterion\n            inst_weight = 0.0\n        \n        model, history = train_model(\n            model, optimizer, scheduler, train_loader, val_loader, transmil_criterion,\n            max_epochs=max_epochs, patience=patience, name=f'{model_name}_{rtag}',\n            inst_loss_weight=inst_weight\n        )\n        \n        # 평가\n        val_results = evaluate(model, val_loader, transmil_criterion)\n        test_results = evaluate(model, test_loader, transmil_criterion)\n        \n        # 결과 저장\n        matched_results[rtag]['models'][model_name] = {\n            'model': model,\n            'history': history,\n            'val': val_results,\n            'test': test_results\n        }\n        \n        print(f\"  ✅ {model_name} 완료: Val AUC={val_results['auc']:.3f}, Test AUC={test_results['auc']:.3f}\")\n        print(f\"     Test Recall={test_results['recall']:.3f}, Test F1={test_results['f1']:.3f}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"🏆 Matched 모드 실험 완료!\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shift_experiments",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T10:56:27.197224Z",
     "start_time": "2025-09-14T10:56:02.095511Z"
    }
   },
   "outputs": [],
   "source": "# ==============================================================================\n# Shift 모드 실험: 30% 학습 → 다양한 비율 평가\n# ==============================================================================\n\nprint(\"\\n🔄 Shift 모드 실험 시작\")\nprint(\"=\" * 80)\nprint(\"30% 위조 비율로 학습한 모델을 다양한 비율(5/10/20/30/50%)로 평가합니다.\")\nprint(\"목표: 도메인 적응성 및 일반화 능력 분석\")\nprint(\"=\" * 80)\n\n# 30% 학습 데이터로 DataLoader 생성\nshift_features = {\n    'train': shift_data['train_features'],\n    'val': shift_data['val_features']\n}\nshift_labels = {\n    'train': shift_data['train']['labels'],\n    'val': shift_data['val']['labels']\n}\n\nshift_train_loader = DataLoader(MILDataset(shift_features['train'], shift_labels['train']), \n                               batch_size=batch_size, shuffle=True)\nshift_val_loader = DataLoader(MILDataset(shift_features['val'], shift_labels['val']), \n                             batch_size=batch_size, shuffle=False)\n\nprint(f\"학습 데이터: 30% 위조 비율, Train={len(shift_labels['train'])}, Val={len(shift_labels['val'])}\")\n\n# 결과 저장\nshift_results = {'train_ratio': shift_data['train_ratio'], 'models': {}}\n\n# 각 모델별로 30% 데이터로 학습 (총 5개 모델)\nfor model_name, model_class in MODEL_CLASSES.items():\n    print(f\"\\n🚀 {model_name} 학습 중... (30% 위조 비율)\")\n    \n    # 시드 고정\n    seed_everything(42)\n    \n    # 모델 생성 (TransMIL 특별 처리)\n    if model_name == 'MeanPooling':\n        model = model_class(input_dim=256).to(device)\n    elif model_name == 'TransMIL':\n        model = model_class(\n            input_dim=256, \n            embed_dim=512, \n            num_heads=8, \n            dropout_p=0.1, \n            use_1d_ppeg=True\n        ).to(device)\n    else:\n        model = model_class(input_dim=256, hidden_dim=128, dropout_p=0.1).to(device)\n    \n    # Optimizer & Scheduler (TransMIL은 AdamW 사용)\n    if model_name == 'TransMIL':\n        optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-2)\n    else:\n        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n    \n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='max', factor=0.5, patience=scheduler_patience, verbose=False\n    )\n    \n    # 30% 데이터로 학습 (TransMIL은 ClassWeightedBCE 사용)\n    if model_name == 'TransMIL':\n        transmil_criterion = ClassWeightedBCE(pos_weight=1.0, neg_weight=2.0)\n        inst_weight = 0.0\n    elif model_name == 'DSMIL':\n        transmil_criterion = criterion\n        inst_weight = 0.5\n    else:\n        transmil_criterion = criterion\n        inst_weight = 0.0\n    \n    model, history = train_model(\n        model, optimizer, scheduler, shift_train_loader, shift_val_loader, transmil_criterion,\n        max_epochs=max_epochs, patience=patience, name=f'{model_name}_shift',\n        inst_loss_weight=inst_weight\n    )\n    \n    # 다양한 비율로 평가\n    eval_results = {}\n    print(f\"  📊 {model_name} 다양한 비율 평가 중...\")\n    \n    for eval_rtag, eval_data in shift_data['eval_sets'].items():\n        eval_ratio = eval_data['ratio']\n        test_loader = DataLoader(MILDataset(eval_data['test_features'], eval_data['test']['labels']), \n                                batch_size=batch_size, shuffle=False)\n        \n        test_results = evaluate(model, test_loader, transmil_criterion)\n        eval_results[eval_rtag] = {\n            'ratio': eval_ratio,\n            'test': test_results\n        }\n        \n        print(f\"    {eval_rtag} ({eval_ratio:.0%}): AUC={test_results['auc']:.3f}, Recall={test_results['recall']:.3f}\")\n    \n    # 결과 저장\n    shift_results['models'][model_name] = {\n        'model': model,\n        'history': history,\n        'evaluations': eval_results\n    }\n    \n    print(f\"  ✅ {model_name} Shift 실험 완료\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"🏆 Shift 모드 실험 완료!\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "results_analysis",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T10:56:27.214631Z",
     "start_time": "2025-09-14T10:56:27.201518Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 위조 비율 실험 결과 종합 분석\n",
      "================================================================================\n",
      "\n",
      "1️⃣ Matched 모드 결과 (각 비율별 독립 학습)\n",
      "--------------------------------------------------------------------------------\n",
      "비율       모델           AUC      F1       Precision   Recall   Accuracy  \n",
      "--------------------------------------------------------------------------------\n",
      "5%     AttentionMIL 0.626    0.000    0.000       0.000    0.500     \n",
      "5%     DSMIL        0.633    0.036    0.611       0.018    0.503     \n",
      "5%     MeanPooling  0.508    0.000    0.000       0.000    0.500     \n",
      "10%     AttentionMIL 0.644    0.003    1.000       0.002    0.501     \n",
      "10%     DSMIL        0.679    0.405    0.758       0.277    0.594     \n",
      "10%     MeanPooling  0.503    0.000    0.000       0.000    0.500     \n",
      "20%     AttentionMIL 0.779    0.586    0.766       0.475    0.665     \n",
      "20%     DSMIL        0.799    0.721    0.574       0.970    0.625     \n",
      "20%     MeanPooling  0.522    0.000    0.000       0.000    0.500     \n",
      "30%     AttentionMIL 0.804    0.745    0.708       0.787    0.731     \n",
      "30%     DSMIL        0.845    0.742    0.597       0.980    0.659     \n",
      "30%     MeanPooling  0.503    0.000    0.000       0.000    0.500     \n",
      "50%     AttentionMIL 0.834    0.695    0.808       0.610    0.733     \n",
      "50%     DSMIL        0.843    0.734    0.584       0.987    0.642     \n",
      "50%     MeanPooling  0.521    0.000    0.000       0.000    0.500     \n",
      "\n",
      "\n",
      "2️⃣ Shift 모드 결과 (30% 학습 → 다양한 비율 평가)\n",
      "--------------------------------------------------------------------------------\n",
      "평가비율     모델           AUC      F1       Precision   Recall   Accuracy  \n",
      "--------------------------------------------------------------------------------\n",
      "5%       AttentionMIL 0.639    0.608    0.596       0.620    0.600     \n",
      "10%       AttentionMIL 0.648    0.614    0.599       0.628    0.604     \n",
      "20%       AttentionMIL 0.790    0.740    0.675       0.818    0.713     \n",
      "30%       AttentionMIL 0.802    0.743    0.667       0.840    0.710     \n",
      "50%       AttentionMIL 0.825    0.759    0.688       0.847    0.732     \n",
      "5%       DSMIL        0.639    0.655    0.561       0.787    0.585     \n",
      "10%       DSMIL        0.647    0.676    0.580       0.812    0.612     \n",
      "20%       DSMIL        0.790    0.731    0.603       0.928    0.659     \n",
      "30%       DSMIL        0.814    0.755    0.623       0.960    0.689     \n",
      "50%       DSMIL        0.833    0.741    0.609       0.947    0.669     \n",
      "5%       MeanPooling  0.511    0.000    0.000       0.000    0.500     \n",
      "10%       MeanPooling  0.496    0.000    0.000       0.000    0.500     \n",
      "20%       MeanPooling  0.507    0.000    0.000       0.000    0.500     \n",
      "30%       MeanPooling  0.515    0.000    0.000       0.000    0.500     \n",
      "50%       MeanPooling  0.523    0.000    0.000       0.000    0.500     \n",
      "\n",
      "✅ 결과 분석 데이터 준비 완료\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 위조 비율 실험 결과 종합 분석\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"📊 위조 비율 실험 결과 종합 분석\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Matched 모드 결과 요약\n",
    "print(\"\\n1️⃣ Matched 모드 결과 (각 비율별 독립 학습)\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'비율':<8} {'모델':<12} {'AUC':<8} {'F1':<8} {'Precision':<11} {'Recall':<8} {'Accuracy':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "matched_summary = []\n",
    "for rtag, data in matched_results.items():\n",
    "    ratio = data['ratio']\n",
    "    for model_name, results in data['models'].items():\n",
    "        test = results['test']\n",
    "        print(f\"{ratio:.0%}     {model_name:<12} {test['auc']:<8.3f} {test['f1']:<8.3f} \"\n",
    "              f\"{test['precision']:<11.3f} {test['recall']:<8.3f} {test['accuracy']:<10.3f}\")\n",
    "        \n",
    "        matched_summary.append({\n",
    "            'ratio': ratio,\n",
    "            'model': model_name,\n",
    "            'auc': test['auc'],\n",
    "            'f1': test['f1'],\n",
    "            'precision': test['precision'],\n",
    "            'recall': test['recall'],\n",
    "            'accuracy': test['accuracy']\n",
    "        })\n",
    "\n",
    "# 2. Shift 모드 결과 요약\n",
    "print(\"\\n\\n2️⃣ Shift 모드 결과 (30% 학습 → 다양한 비율 평가)\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'평가비율':<8} {'모델':<12} {'AUC':<8} {'F1':<8} {'Precision':<11} {'Recall':<8} {'Accuracy':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "shift_summary = []\n",
    "for model_name, model_results in shift_results['models'].items():\n",
    "    for eval_rtag, eval_results in model_results['evaluations'].items():\n",
    "        ratio = eval_results['ratio']\n",
    "        test = eval_results['test']\n",
    "        print(f\"{ratio:.0%}       {model_name:<12} {test['auc']:<8.3f} {test['f1']:<8.3f} \"\n",
    "              f\"{test['precision']:<11.3f} {test['recall']:<8.3f} {test['accuracy']:<10.3f}\")\n",
    "        \n",
    "        shift_summary.append({\n",
    "            'ratio': ratio,\n",
    "            'model': model_name,\n",
    "            'auc': test['auc'],\n",
    "            'f1': test['f1'],\n",
    "            'precision': test['precision'],\n",
    "            'recall': test['recall'],\n",
    "            'accuracy': test['accuracy']\n",
    "        })\n",
    "\n",
    "# DataFrame으로 변환 (분석 편의성)\n",
    "matched_df = pd.DataFrame(matched_summary)\n",
    "shift_df = pd.DataFrame(shift_summary)\n",
    "\n",
    "print(\"\\n✅ 결과 분석 데이터 준비 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "performance_visualization",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:05:59.934733Z",
     "start_time": "2025-09-14T11:05:57.945277Z"
    }
   },
   "outputs": [],
   "source": "# ==============================================================================\n# 성능 시각화 및 핵심 인사이트 분석\n# ==============================================================================\n\n# 1. Matched 모드 성능 변화 시각화\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\nfig.suptitle('Matched Mode: Performance by Forgery Ratio', fontsize=16, fontweight='bold')\n\nmetrics = ['auc', 'recall', 'f1', 'precision']\nmetric_names = ['AUC', 'Recall', 'F1', 'Precision']\n\nfor i, (metric, metric_name) in enumerate(zip(metrics, metric_names)):\n    ax = axes[i//2, i%2]\n    \n    # Matched 모드만 플롯 (5개 모델)\n    for j, model in enumerate(['AttentionMIL', 'GatedAttentionMIL', 'DSMIL', 'TransMIL', 'MeanPooling']):\n        model_data = matched_df[matched_df['model'] == model]\n        colors = ['blue', 'orange', 'red', 'purple', 'green']\n        ax.plot(model_data['ratio'], model_data[metric], 'o-', \n               color=colors[j], label=model, linewidth=2, markersize=6)\n    \n    ax.set_xlabel('Forgery Ratio')\n    ax.set_ylabel(metric_name)\n    ax.set_title(f'{metric_name} Performance (Matched)')\n    ax.grid(True, alpha=0.3)\n    ax.legend(fontsize=9)\n    \n    # x축 라벨을 퍼센트로 표시 (50% 포함)\n    ax.set_xticks([0.05, 0.10, 0.20, 0.30, 0.50])\n    ax.set_xticklabels(['5%', '10%', '20%', '30%', '50%'])\n\nplt.tight_layout()\nplt.show()\n\n# 2. Shift 모드 성능 변화 시각화\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\nfig.suptitle('Shift Mode: Performance by Forgery Ratio (Trained on 30%)', fontsize=16, fontweight='bold')\n\nfor i, (metric, metric_name) in enumerate(zip(metrics, metric_names)):\n    ax = axes[i//2, i%2]\n    \n    # Shift 모드만 플롯 (5개 모델)\n    for j, model in enumerate(['AttentionMIL', 'GatedAttentionMIL', 'DSMIL', 'TransMIL', 'MeanPooling']):\n        model_data = shift_df[shift_df['model'] == model]\n        colors = ['blue', 'orange', 'red', 'purple', 'green']\n        ax.plot(model_data['ratio'], model_data[metric], 's--', \n               color=colors[j], label=model, linewidth=2, markersize=6, alpha=0.8)\n    \n    ax.set_xlabel('Evaluation Forgery Ratio')\n    ax.set_ylabel(metric_name)\n    ax.set_title(f'{metric_name} Performance (Shift)')\n    ax.grid(True, alpha=0.3)\n    ax.legend(fontsize=9)\n    \n    # x축 라벨을 퍼센트로 표시 (50% 포함)\n    ax.set_xticks([0.05, 0.10, 0.20, 0.30, 0.50])\n    ax.set_xticklabels(['5%', '10%', '20%', '30%', '50%'])\n\nplt.tight_layout()\nplt.show()\n\n# 3. Matched vs Shift 직접 비교 (AUC와 Recall만)\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\nfig.suptitle('Matched vs Shift Comparison', fontsize=16, fontweight='bold')\n\ncomparison_metrics = ['auc', 'recall']\ncomparison_names = ['AUC', 'Recall']\n\nfor i, (metric, metric_name) in enumerate(zip(comparison_metrics, comparison_names)):\n    ax = axes[i]\n    \n    # AttentionMIL, GatedAttentionMIL, DSMIL, TransMIL만 비교 (MeanPooling 제외)\n    for j, model in enumerate(['AttentionMIL', 'GatedAttentionMIL', 'DSMIL', 'TransMIL']):\n        matched_data = matched_df[matched_df['model'] == model]\n        shift_data = shift_df[shift_df['model'] == model]\n        colors = ['blue', 'orange', 'red', 'purple']\n        \n        ax.plot(matched_data['ratio'], matched_data[metric], 'o-', \n               color=colors[j], label=f'{model} (Matched)', linewidth=2, markersize=6)\n        ax.plot(shift_data['ratio'], shift_data[metric], 's--', \n               color=colors[j], label=f'{model} (Shift)', linewidth=2, markersize=6, alpha=0.7)\n    \n    ax.set_xlabel('Forgery Ratio')\n    ax.set_ylabel(metric_name)\n    ax.set_title(f'{metric_name}: Matched vs Shift')\n    ax.grid(True, alpha=0.3)\n    ax.legend(fontsize=8)\n    \n    # x축 라벨을 퍼센트로 표시 (50% 포함)\n    ax.set_xticks([0.05, 0.10, 0.20, 0.30, 0.50])\n    ax.set_xticklabels(['5%', '10%', '20%', '30%', '50%'])\n\nplt.tight_layout()\nplt.show()\n\n# 2. 낮은 위조 비율에서의 성능 저하 분석\nprint(\"\\n🔍 핵심 인사이트 분석\")\nprint(\"=\" * 60)\n\n# 30% vs 5% 성능 비교 (Matched 모드)\nprint(\"\\n📉 성능 저하 분석 (30% → 5%, Matched 모드)\")\nprint(\"-\" * 50)\nfor model in ['AttentionMIL', 'GatedAttentionMIL', 'DSMIL', 'TransMIL', 'MeanPooling']:\n    perf_30 = matched_df[(matched_df['model'] == model) & (matched_df['ratio'] == 0.30)]\n    perf_05 = matched_df[(matched_df['model'] == model) & (matched_df['ratio'] == 0.05)]\n    \n    if not perf_30.empty and not perf_05.empty:\n        auc_drop = perf_30['auc'].iloc[0] - perf_05['auc'].iloc[0]\n        recall_drop = perf_30['recall'].iloc[0] - perf_05['recall'].iloc[0]\n        \n        print(f\"{model}:\")\n        print(f\"  AUC: {perf_30['auc'].iloc[0]:.3f} → {perf_05['auc'].iloc[0]:.3f} (Δ{auc_drop:+.3f})\")\n        print(f\"  Recall: {perf_30['recall'].iloc[0]:.3f} → {perf_05['recall'].iloc[0]:.3f} (Δ{recall_drop:+.3f})\")\n\n# 3. Matched vs Shift 비교 (5% 평가 기준)\nprint(\"\\n🆚 Matched vs Shift 비교 (5% 평가 기준)\")\nprint(\"-\" * 50)\nfor model in ['AttentionMIL', 'GatedAttentionMIL', 'DSMIL', 'TransMIL', 'MeanPooling']:\n    matched_05 = matched_df[(matched_df['model'] == model) & (matched_df['ratio'] == 0.05)]\n    shift_05 = shift_df[(shift_df['model'] == model) & (shift_df['ratio'] == 0.05)]\n    \n    if not matched_05.empty and not shift_05.empty:\n        auc_diff = shift_05['auc'].iloc[0] - matched_05['auc'].iloc[0]\n        recall_diff = shift_05['recall'].iloc[0] - matched_05['recall'].iloc[0]\n        \n        print(f\"{model}:\")\n        print(f\"  AUC: Matched={matched_05['auc'].iloc[0]:.3f}, Shift={shift_05['auc'].iloc[0]:.3f} (Δ{auc_diff:+.3f})\")\n        print(f\"  Recall: Matched={matched_05['recall'].iloc[0]:.3f}, Shift={shift_05['recall'].iloc[0]:.3f} (Δ{recall_diff:+.3f})\")\n\n# 4. 최고 성능 모델 찾기\nprint(\"\\n🏆 최고 성능 모델 (5% 위조 비율 기준)\")\nprint(\"-\" * 50)\n\n# Matched 모드에서 5% 최고 성능\nmatched_05_best = matched_df[matched_df['ratio'] == 0.05].sort_values('auc', ascending=False)\nif not matched_05_best.empty:\n    best = matched_05_best.iloc[0]\n    print(f\"Matched 모드: {best['model']} (AUC: {best['auc']:.3f}, Recall: {best['recall']:.3f})\")\n\n# Shift 모드에서 5% 최고 성능\nshift_05_best = shift_df[shift_df['ratio'] == 0.05].sort_values('auc', ascending=False)\nif not shift_05_best.empty:\n    best = shift_05_best.iloc[0]\n    print(f\"Shift 모드: {best['model']} (AUC: {best['auc']:.3f}, Recall: {best['recall']:.3f})\")\n\nprint(\"\\n✅ 시각화 및 분석 완료!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conclusions",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T10:56:28.001657Z",
     "start_time": "2025-09-14T10:56:27.979195Z"
    }
   },
   "outputs": [],
   "source": "# ==============================================================================\n# 결론 및 핵심 발견사항\n# ==============================================================================\n\nprint(\"📋 위조 비율 실험 결론\")\nprint(\"=\" * 80)\n\nprint(\"\\n🎯 핵심 연구 질문 답변:\")\nprint(\"\\n1. 위조 비율이 낮아질수록 성능이 얼마나 저하되는가?\")\n\n# 평균 성능 저하 계산\navg_drops = {}\nfor model in ['AttentionMIL', 'GatedAttentionMIL', 'DSMIL', 'TransMIL', 'MeanPooling']:\n    perf_30 = matched_df[(matched_df['model'] == model) & (matched_df['ratio'] == 0.30)]\n    perf_05 = matched_df[(matched_df['model'] == model) & (matched_df['ratio'] == 0.05)]\n    \n    if not perf_30.empty and not perf_05.empty:\n        auc_drop = (perf_30['auc'].iloc[0] - perf_05['auc'].iloc[0]) / perf_30['auc'].iloc[0] * 100\n        recall_drop = (perf_30['recall'].iloc[0] - perf_05['recall'].iloc[0]) / perf_30['recall'].iloc[0] * 100 if perf_30['recall'].iloc[0] > 0 else 0\n        avg_drops[model] = {'auc': auc_drop, 'recall': recall_drop}\n\nfor model, drops in avg_drops.items():\n    print(f\"   • {model}: AUC {drops['auc']:.1f}% 저하, Recall {drops['recall']:.1f}% 저하\")\n\nprint(\"\\n2. 어떤 모델이 낮은 위조 비율에서 더 강건한가?\")\nbest_05_models = matched_df[matched_df['ratio'] == 0.05].sort_values(['recall', 'auc'], ascending=False)\nprint(f\"   • 5% 비율에서 최고 성능: {best_05_models.iloc[0]['model']}\")\nprint(f\"     Recall: {best_05_models.iloc[0]['recall']:.3f}, AUC: {best_05_models.iloc[0]['auc']:.3f}\")\n\nprint(\"\\n3. 30% 학습 모델이 5% 테스트에서도 잘 동작하는가? (도메인 적응)\")\ndomain_adaptation_analysis = []\nfor model in ['AttentionMIL', 'GatedAttentionMIL', 'DSMIL', 'TransMIL', 'MeanPooling']:\n    matched_05 = matched_df[(matched_df['model'] == model) & (matched_df['ratio'] == 0.05)]\n    shift_05 = shift_df[(shift_df['model'] == model) & (shift_df['ratio'] == 0.05)]\n    \n    if not matched_05.empty and not shift_05.empty:\n        performance_ratio = shift_05['auc'].iloc[0] / matched_05['auc'].iloc[0] if matched_05['auc'].iloc[0] > 0 else 0\n        domain_adaptation_analysis.append((model, performance_ratio))\n        status = \"우수\" if performance_ratio > 0.95 else \"보통\" if performance_ratio > 0.85 else \"저조\"\n        print(f\"   • {model}: {performance_ratio:.2%} 성능 유지 ({status})\")\n\nprint(\"\\n📊 실험 요약:\")\nprint(f\"   • 총 {len(RATIOS)}개 위조 비율 테스트: {[f'{r:.0%}' for r in RATIOS]}\")\nprint(f\"   • {len(MODEL_CLASSES)}개 모델 비교: {list(MODEL_CLASSES.keys())}\")\nprint(f\"   • 2가지 실험 모드: Matched (독립 학습) vs Shift (도메인 적응)\")\n\n# 최종 권장사항\nprint(\"\\n💡 권장사항:\")\nbest_overall = matched_df.groupby('model')['recall'].mean().sort_values(ascending=False)\nprint(f\"   • 전반적 권장 모델: {best_overall.index[0]} (평균 Recall: {best_overall.iloc[0]:.3f})\")\nprint(f\"   • 낮은 위조 비율 전용: {best_05_models.iloc[0]['model']}\")\n\nif any(ratio > 0.95 for _, ratio in domain_adaptation_analysis):\n    best_adapt_model = max(domain_adaptation_analysis, key=lambda x: x[1])[0]\n    print(f\"   • 도메인 적응성 우수: {best_adapt_model}\")\nelse:\n    print(f\"   • 도메인 적응: 모든 모델에서 성능 저하 관찰, Matched 모드 권장\")\n\nprint(\"\\n🔍 TransMIL 추가로 얻은 인사이트:\")\nprint(\"   • Transformer 기반 모델의 위조 비율 적응성 평가\")\nprint(\"   • Self-attention mechanism의 MIL 성능 기여도 분석\")\nprint(\"   • 다양한 아키텍처(CNN, RNN, Transformer) 간 성능 비교\")\n\nprint(\"\\n🔚 위조 비율 실험 완료 (총 5개 모델)\")\nprint(\"=\"*80)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}