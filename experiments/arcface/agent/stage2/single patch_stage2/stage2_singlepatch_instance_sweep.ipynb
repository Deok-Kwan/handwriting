{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1895d3ac",
   "metadata": {},
   "source": [
    "# Stage 2 Sweep: Single Patch MIL Bags\n",
    "\n",
    "혼합비 30%를 고정한 채 bag당 인스턴스 수를 변경하면서 Stage 2 데이터를 반복 생성합니다. 실험 구성을 파이썬 함수로 정리해 한 번의 실행으로 여러 세트를 만들 수 있도록 구성했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28c28d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from dataclasses import dataclass, replace\n",
    "from typing import Any, Dict, List, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "INSTANCES_PER_BAG_GRID = [10, 20, 30, 40, 50]\n",
    "SEED_OFFSETS = {\"train\": 0, \"val\": 10, \"test\": 20}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Stage2Config:\n",
    "    margin_value: str = \"0.4\"\n",
    "    embedding_dir: str = \"/workspace/MIL/data/processed/embeddings\"\n",
    "    bags_dir: str = \"/workspace/MIL/data/processed/bags\"\n",
    "    raw_meta_csv: str = \"/workspace/MIL/data/raw/naver_ocr.csv\"\n",
    "    seed_base: int = 42\n",
    "    win: int = 1\n",
    "    stride: int = 1\n",
    "    instances_per_bag: int = 30\n",
    "    min_partner_instances: int = 1\n",
    "    mix_instance_ratio: float = 0.30\n",
    "    tokens_negative: int = 60\n",
    "    tokens_positive: int = 60\n",
    "    total_bags_per_writer: int = 20\n",
    "    target_positive_ratio: float = 0.30\n",
    "    positive_order: str = \"shuffle\"\n",
    "    baseline_suffix: str = \"baseline\"\n",
    "    compat_suffix: str = \"random\"\n",
    "\n",
    "    def for_instances(self, instances: int) -> \"Stage2Config\":\n",
    "        tokens_neg = max(self.tokens_negative, instances)\n",
    "        tokens_pos = max(self.tokens_positive, instances)\n",
    "        return replace(\n",
    "            self,\n",
    "            instances_per_bag=instances,\n",
    "            tokens_negative=tokens_neg,\n",
    "            tokens_positive=tokens_pos,\n",
    "        )\n",
    "\n",
    "\n",
    "config = Stage2Config()\n",
    "\n",
    "\n",
    "def setup_environment(seed: int) -> None:\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    cuda_default = os.environ.get(\"CUDA_VISIBLE_DEVICES\")\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = os.getenv(\"MIL_STAGE2_GPU\", cuda_default or \"4\")\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "setup_environment(config.seed_base)\n",
    "os.makedirs(config.bags_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7770127",
   "metadata": {},
   "outputs": [],
   "source": "def pick_writer_col(df: pd.DataFrame) -> str:\n    \"\"\"작성자 식별 컬럼을 안전하게 선택\"\"\"\n    candidates = [\"author_id\", \"writer_id\", \"writer\", \"writerID\", \"author\"]\n    for c in candidates:\n        if c in df.columns:\n            # 최소 10명 이상의 고유 작성자가 있어야 정상\n            if df[c].nunique() >= 10:\n                return c\n    # 최후의 수단으로 'label'을 쓰되, 경고/검증을 건다\n    if \"label\" in df.columns:\n        nunq = df[\"label\"].nunique()\n        if nunq <= 5:\n            raise ValueError(\n                f\"'label'({nunq} uniques)로 작성자를 그룹핑하려고 합니다. \"\n                f\"작성자 식별 컬럼(author_id/writer_id 등)을 CSV에 포함시키세요.\"\n            )\n        return \"label\"\n    raise ValueError(\"작성자 식별 컬럼(author_id/writer_id 등)을 찾을 수 없습니다.\")\n\n\ndef load_split_csv(config: Stage2Config, split: str) -> Tuple[pd.DataFrame, str, List[str]]:\n    path = os.path.join(\n        config.embedding_dir,\n        f\"mil_arcface_margin_{config.margin_value}_{split}_data.csv\",\n    )\n    df = pd.read_csv(path)\n    writer_col = pick_writer_col(df)\n    emb_cols = [c for c in df.columns if c.startswith(\"embedding\")]\n    if not emb_cols:\n        raise ValueError(f\"No embedding_* columns found in {path}\")\n    return df, writer_col, emb_cols\n\n\ndef load_splits(config: Stage2Config):\n    train_df, writer_col, emb_cols = load_split_csv(config, \"train\")\n    val_df, _, _ = load_split_csv(config, \"val\")\n    test_df, _, _ = load_split_csv(config, \"test\")\n    return {\"train\": train_df, \"val\": val_df, \"test\": test_df}, writer_col, emb_cols\n\n\ndef build_writer_index(df: pd.DataFrame, writer_col: str, emb_cols: Sequence[str]) -> Dict[int, Dict[str, Any]]:\n    store: Dict[int, Dict[str, Any]] = {}\n    has_path = \"path\" in df.columns\n    for wid, group in df.groupby(writer_col):\n        key = int(wid)\n        store[key] = {\n            \"emb\": group[emb_cols].to_numpy(dtype=np.float32),\n            \"paths\": group[\"path\"].tolist() if has_path else [\"\"] * len(group),\n            \"idx\": group.index.to_list(),\n        }\n    return store\n\n\ndef build_writer_indices(frames, writer_col: str, emb_cols: Sequence[str]):\n    return {\n        split: build_writer_index(df, writer_col, emb_cols)\n        for split, df in frames.items()\n    }\n\n\nframes, writer_col, emb_cols = load_splits(config)\nembed_dim = len(emb_cols)\nwriter_indices = build_writer_indices(frames, writer_col, emb_cols)\n\nprint(\"Loaded ArcFace splits:\")\nfor split_key, df in frames.items():\n    print(f\"  {split_key.capitalize():5s}: {len(df)} rows\")\nprint(f\"Embedding dimension: {embed_dim}, writer column: {writer_col}\")\n\ntry:\n    raw_count = len(pd.read_csv(config.raw_meta_csv))\n    print(f\"Raw metadata rows: {raw_count}\")\nexcept Exception:\n    print(\"Raw metadata not available or failed to load.\")\n\nprint(\"Writer counts:\")\nfor split_key, store in writer_indices.items():\n    print(f\"  {split_key.capitalize():5s}: {len(store)} writers\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b252b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_k(n: int, k: int, rng: np.random.Generator, replace_if_needed: bool = True) -> List[int]:\n",
    "    if n >= k:\n",
    "        return rng.choice(n, size=k, replace=False).tolist()\n",
    "    if replace_if_needed:\n",
    "        return rng.choice(n, size=k, replace=True).tolist()\n",
    "    return rng.choice(n, size=n, replace=False).tolist()\n",
    "\n",
    "\n",
    "def pack(word_indices: Sequence[int], wid: int, store: Dict[str, Any]) -> List[Tuple[np.ndarray, int, str, int]]:\n",
    "    emb = store[\"emb\"]\n",
    "    paths = store[\"paths\"]\n",
    "    idxs = store[\"idx\"]\n",
    "    return [(emb[i], wid, paths[i], int(idxs[i])) for i in word_indices]\n",
    "\n",
    "\n",
    "def sliding_windows(\n",
    "    seq: Sequence[Tuple[np.ndarray, int, str, int]], win: int, stride: int\n",
    ") -> Tuple[List[np.ndarray], List[Dict[str, Any]]]:\n",
    "    if len(seq) < win:\n",
    "        return [], []\n",
    "    windows: List[np.ndarray] = []\n",
    "    metas: List[Dict[str, Any]] = []\n",
    "    for start in range(0, len(seq) - win + 1, stride):\n",
    "        chunk = seq[start : start + win]\n",
    "        windows.append(np.stack([item[0] for item in chunk], axis=0))\n",
    "        metas.append({\n",
    "            \"window_idx\": start,\n",
    "            \"word_indices\": [item[3] for item in chunk],\n",
    "            \"word_paths\": [item[2] for item in chunk],\n",
    "            \"writer_ids\": [item[1] for item in chunk],\n",
    "        })\n",
    "    return windows, metas\n",
    "\n",
    "\n",
    "def resolve_instance_counts(instances_per_bag: int, mix_ratio: float, min_partner: int) -> Tuple[int, int]:\n",
    "    partner = max(min_partner, int(round(instances_per_bag * mix_ratio)))\n",
    "    partner = min(partner, instances_per_bag - min_partner)\n",
    "    anchor = instances_per_bag - partner\n",
    "    return anchor, partner\n",
    "\n",
    "\n",
    "def make_negative_bag(\n",
    "    wid: int,\n",
    "    store: Dict[str, Any],\n",
    "    rng: np.random.Generator,\n",
    "    cfg: Stage2Config,\n",
    ") -> Tuple[np.ndarray, List[Dict[str, Any]], List[int]]:\n",
    "    sel = sample_k(len(store[\"emb\"]), cfg.tokens_negative, rng, replace_if_needed=True)\n",
    "    seq = pack(sel, wid, store)\n",
    "    wins, metas = sliding_windows(seq, cfg.win, cfg.stride)\n",
    "    if not wins:\n",
    "        raise ValueError(f\"No windows available for writer {wid}\")\n",
    "    if len(wins) >= cfg.instances_per_bag:\n",
    "        selected = rng.choice(len(wins), size=cfg.instances_per_bag, replace=False).tolist()\n",
    "    else:\n",
    "        selected = rng.choice(len(wins), size=cfg.instances_per_bag, replace=True).tolist()\n",
    "    bag = np.stack([wins[i] for i in selected], axis=0)\n",
    "    inst_meta: List[Dict[str, Any]] = []\n",
    "    for i in selected:\n",
    "        meta = dict(metas[i])\n",
    "        meta[\"source_writer\"] = int(wid)\n",
    "        inst_meta.append(meta)\n",
    "    return bag, inst_meta, [int(wid)]\n",
    "\n",
    "\n",
    "def make_pure_windows_for_writer(\n",
    "    wid: int,\n",
    "    store: Dict[str, Any],\n",
    "    rng: np.random.Generator,\n",
    "    tokens_to_sample: int,\n",
    "    cfg: Stage2Config,\n",
    ") -> Tuple[List[np.ndarray], List[Dict[str, Any]]]:\n",
    "    sel = sample_k(len(store[\"emb\"]), tokens_to_sample, rng, replace_if_needed=True)\n",
    "    seq = pack(sel, wid, store)\n",
    "    return sliding_windows(seq, cfg.win, cfg.stride)\n",
    "\n",
    "\n",
    "def make_positive_bag(\n",
    "    wid_anchor: int,\n",
    "    wid_partner: int,\n",
    "    anchor_store: Dict[str, Any],\n",
    "    partner_store: Dict[str, Any],\n",
    "    rng: np.random.Generator,\n",
    "    cfg: Stage2Config,\n",
    ") -> Tuple[np.ndarray, List[Dict[str, Any]], List[int]]:\n",
    "    anchor_wins, anchor_meta = make_pure_windows_for_writer(\n",
    "        wid_anchor, anchor_store, rng, cfg.tokens_positive, cfg\n",
    "    )\n",
    "    partner_wins, partner_meta = make_pure_windows_for_writer(\n",
    "        wid_partner, partner_store, rng, cfg.tokens_positive, cfg\n",
    "    )\n",
    "\n",
    "    anchor_count, partner_count = resolve_instance_counts(\n",
    "        cfg.instances_per_bag, cfg.mix_instance_ratio, cfg.min_partner_instances\n",
    "    )\n",
    "\n",
    "    def pick_k(\n",
    "        wins: Sequence[np.ndarray],\n",
    "        metas: Sequence[Dict[str, Any]],\n",
    "        k: int,\n",
    "    ) -> Tuple[List[np.ndarray], List[Dict[str, Any]]]:\n",
    "        if not wins:\n",
    "            raise ValueError(\"Positive bag sampling failed: empty window list\")\n",
    "        if len(wins) >= k:\n",
    "            idx = rng.choice(len(wins), size=k, replace=False).tolist()\n",
    "        else:\n",
    "            idx = rng.choice(len(wins), size=k, replace=True).tolist()\n",
    "        return [wins[i] for i in idx], [metas[i] for i in idx]\n",
    "\n",
    "    anchor_sel, anchor_meta_sel = pick_k(anchor_wins, anchor_meta, anchor_count)\n",
    "    partner_sel, partner_meta_sel = pick_k(partner_wins, partner_meta, partner_count)\n",
    "\n",
    "    def annotate(meta_list: Sequence[Dict[str, Any]], wid: int) -> List[Dict[str, Any]]:\n",
    "        annotated: List[Dict[str, Any]] = []\n",
    "        for meta in meta_list:\n",
    "            new_meta = dict(meta)\n",
    "            new_meta[\"source_writer\"] = int(wid)\n",
    "            annotated.append(new_meta)\n",
    "        return annotated\n",
    "\n",
    "    anchor_meta_sel = annotate(anchor_meta_sel, wid_anchor)\n",
    "    partner_meta_sel = annotate(partner_meta_sel, wid_partner)\n",
    "\n",
    "    order = cfg.positive_order.lower()\n",
    "    if order == \"a5b5\":\n",
    "        seq_w = anchor_sel + partner_sel\n",
    "        seq_m = anchor_meta_sel + partner_meta_sel\n",
    "    elif order == \"abab\":\n",
    "        seq_w: List[np.ndarray] = []\n",
    "        seq_m: List[Dict[str, Any]] = []\n",
    "        pairable = min(len(anchor_sel), len(partner_sel))\n",
    "        for i in range(pairable):\n",
    "            seq_w.extend([anchor_sel[i], partner_sel[i]])\n",
    "            seq_m.extend([anchor_meta_sel[i], partner_meta_sel[i]])\n",
    "        if len(anchor_sel) > pairable:\n",
    "            seq_w.extend(anchor_sel[pairable:])\n",
    "            seq_m.extend(anchor_meta_sel[pairable:])\n",
    "        if len(partner_sel) > pairable:\n",
    "            seq_w.extend(partner_sel[pairable:])\n",
    "            seq_m.extend(partner_meta_sel[pairable:])\n",
    "    else:\n",
    "        combined = list(zip(anchor_sel + partner_sel, anchor_meta_sel + partner_meta_sel))\n",
    "        rng.shuffle(combined)\n",
    "        seq_w = [item[0] for item in combined]\n",
    "        seq_m = [item[1] for item in combined]\n",
    "\n",
    "    if len(seq_w) < cfg.instances_per_bag:\n",
    "        raise ValueError(\"Positive bag assembly produced insufficient instances.\")\n",
    "    seq_w = seq_w[: cfg.instances_per_bag]\n",
    "    seq_m = seq_m[: cfg.instances_per_bag]\n",
    "\n",
    "    bag_tensor = np.stack(seq_w, axis=0)\n",
    "    return bag_tensor, seq_m, [int(wid_anchor), int(wid_partner)]\n",
    "\n",
    "\n",
    "def generate_split(\n",
    "    name: str,\n",
    "    writer_index: Dict[int, Dict[str, Any]],\n",
    "    cfg: Stage2Config,\n",
    "    neg_per_writer: int,\n",
    "    pos_per_writer: int,\n",
    "    embed_dim: int,\n",
    "    seed: int,\n",
    ") -> Tuple[List[np.ndarray], List[int], List[Dict[str, Any]]]:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    writer_ids = list(writer_index.keys())\n",
    "\n",
    "    bags: List[np.ndarray] = []\n",
    "    labels: List[int] = []\n",
    "    metadata: List[Dict[str, Any]] = []\n",
    "\n",
    "    for wid in writer_ids:\n",
    "        for _ in range(neg_per_writer):\n",
    "            bag, metas, authors = make_negative_bag(wid, writer_index[wid], rng, cfg)\n",
    "            bags.append(bag)\n",
    "            labels.append(0)\n",
    "            metadata.append({\n",
    "                \"authors\": authors,\n",
    "                \"bag_type\": \"negative\",\n",
    "                \"instances\": metas,\n",
    "                \"anchor_writer_id\": int(wid),\n",
    "                \"partner_writer_id\": None,\n",
    "                \"b_writer\": None,\n",
    "                \"partner_instance_count\": 0,\n",
    "                \"partner_instance_ratio\": 0.0,\n",
    "                \"mix_ratio_target\": 0.0,\n",
    "            })\n",
    "\n",
    "    for wid_anchor in writer_ids:\n",
    "        for _ in range(pos_per_writer):\n",
    "            partner_candidates = [w for w in writer_ids if w != wid_anchor]\n",
    "            if not partner_candidates:\n",
    "                raise ValueError(\"Positive bag requires at least two writers.\")\n",
    "            wid_partner = int(rng.choice(partner_candidates))\n",
    "            bag, metas, authors = make_positive_bag(\n",
    "                wid_anchor,\n",
    "                wid_partner,\n",
    "                writer_index[wid_anchor],\n",
    "                writer_index[wid_partner],\n",
    "                rng,\n",
    "                cfg,\n",
    "            )\n",
    "            partner_writer = int(wid_partner)\n",
    "            partner_instances = sum(1 for meta in metas if meta.get(\"source_writer\") == partner_writer)\n",
    "            partner_ratio = partner_instances / len(metas) if metas else 0.0\n",
    "            bags.append(bag)\n",
    "            labels.append(1)\n",
    "            metadata.append({\n",
    "                \"authors\": authors,\n",
    "                \"bag_type\": \"positive\",\n",
    "                \"instances\": metas,\n",
    "                \"anchor_writer_id\": int(wid_anchor),\n",
    "                \"partner_writer_id\": partner_writer,\n",
    "                \"b_writer\": partner_writer,\n",
    "                \"partner_instance_count\": partner_instances,\n",
    "                \"partner_instance_ratio\": partner_ratio,\n",
    "                \"mix_ratio_target\": cfg.mix_instance_ratio,\n",
    "            })\n",
    "\n",
    "    if not bags:\n",
    "        raise ValueError(f\"No bags generated for split {name}.\")\n",
    "\n",
    "    idx = rng.permutation(len(labels))\n",
    "    bags = [bags[i] for i in idx]\n",
    "    labels = [int(labels[i]) for i in idx]\n",
    "    metadata = [metadata[i] for i in idx]\n",
    "\n",
    "    expected_shape = (cfg.instances_per_bag, cfg.win, embed_dim)\n",
    "    if bags[0].shape != expected_shape:\n",
    "        raise ValueError(f\"Unexpected bag shape {bags[0].shape}, expected {expected_shape}\")\n",
    "\n",
    "    return bags, labels, metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df650514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_ratio_tag(cfg: Stage2Config) -> str:\n",
    "    return f\"{int(round(cfg.mix_instance_ratio * 100)):02d}p_single{cfg.instances_per_bag:02d}\"\n",
    "\n",
    "\n",
    "def save_split(\n",
    "    split_key: str,\n",
    "    payload: Dict[str, Any],\n",
    "    cfg: Stage2Config,\n",
    "    ratio_tag: str,\n",
    "    plain_tag: str,\n",
    ") -> Tuple[str, str, str]:\n",
    "    base_name = f\"bags_arcface_margin_{cfg.margin_value}_{ratio_tag}_{cfg.baseline_suffix}_{split_key}.pkl\"\n",
    "    base_path = os.path.join(cfg.bags_dir, base_name)\n",
    "    with open(base_path, \"wb\") as f:\n",
    "        pickle.dump(payload, f)\n",
    "\n",
    "    alias_name = f\"bags_arcface_margin_{cfg.margin_value}_{ratio_tag}_{cfg.compat_suffix}_{split_key}.pkl\"\n",
    "    alias_path = os.path.join(cfg.bags_dir, alias_name)\n",
    "    with open(alias_path, \"wb\") as f:\n",
    "        pickle.dump(payload, f)\n",
    "\n",
    "    plain_name = f\"bags_arcface_margin_{cfg.margin_value}_{plain_tag}_{cfg.compat_suffix}_{split_key}.pkl\"\n",
    "    plain_path = os.path.join(cfg.bags_dir, plain_name)\n",
    "    with open(plain_path, \"wb\") as f:\n",
    "        pickle.dump(payload, f)\n",
    "\n",
    "    print(f\"    saved {base_name}\")\n",
    "    print(f\"    alias {alias_name}\")\n",
    "    print(f\"    alias {plain_name}\")\n",
    "    return base_path, alias_path, plain_path\n",
    "\n",
    "\n",
    "def summarize_labels(split_key: str, labels: Sequence[int]) -> None:\n",
    "    total = len(labels)\n",
    "    pos = sum(labels)\n",
    "    neg = total - pos\n",
    "    rate = (pos / total * 100) if total else 0.0\n",
    "    print(f\"    {split_key.capitalize():5s}: N={total}, Pos={pos} ({rate:.1f}%), Neg={neg}\")\n",
    "\n",
    "\n",
    "def run_stage2_generation(cfg: Stage2Config, writer_indices, embed_dim: int) -> Dict[str, Tuple[str, str, str]]:\n",
    "    pos_per_writer = int(round(cfg.total_bags_per_writer * cfg.target_positive_ratio))\n",
    "    neg_per_writer = cfg.total_bags_per_writer - pos_per_writer\n",
    "    if pos_per_writer <= 0 or neg_per_writer <= 0:\n",
    "        raise ValueError(\"Adjust total_bags_per_writer/target_positive_ratio to include both bag types.\")\n",
    "\n",
    "    ratio_tag = format_ratio_tag(cfg)\n",
    "    plain_tag = f\"{int(round(cfg.mix_instance_ratio * 100)):02d}p_single{cfg.instances_per_bag:02d}\"\n",
    "    actual_ratio = pos_per_writer / (pos_per_writer + neg_per_writer)\n",
    "\n",
    "    print(f\"\\n=== Stage 2 single-patch generation (instances={cfg.instances_per_bag}) ===\")\n",
    "    print(f\"  mix_ratio={cfg.mix_instance_ratio:.2f} → ratio_tag={ratio_tag}\")\n",
    "    print(f\"  bags per writer: neg={neg_per_writer}, pos={pos_per_writer} (share {actual_ratio * 100:.1f}%)\")\n",
    "\n",
    "    outputs: Dict[str, Dict[str, Any]] = {}\n",
    "    for split_key in (\"train\", \"val\", \"test\"):\n",
    "        split_writers = writer_indices[split_key]\n",
    "        seed = cfg.seed_base + SEED_OFFSETS[split_key]\n",
    "        bags, labels, metadata = generate_split(\n",
    "            split_key.capitalize(),\n",
    "            split_writers,\n",
    "            cfg,\n",
    "            neg_per_writer,\n",
    "            pos_per_writer,\n",
    "            embed_dim,\n",
    "            seed,\n",
    "        )\n",
    "        outputs[split_key] = {\"bags\": bags, \"labels\": labels, \"metadata\": metadata}\n",
    "        print(\n",
    "            f\"  {split_key.capitalize():5s} split → {len(labels)} bags, sample_shape={bags[0].shape}\"\n",
    "        )\n",
    "\n",
    "    print(\"  saving splits...\")\n",
    "    saved_paths: Dict[str, Tuple[str, str, str]] = {}\n",
    "    for split_key, payload in outputs.items():\n",
    "        saved_paths[split_key] = save_split(split_key, payload, cfg, ratio_tag, plain_tag)\n",
    "\n",
    "    print(\"  label summary:\")\n",
    "    for split_key, payload in outputs.items():\n",
    "        summarize_labels(split_key, payload[\"labels\"])\n",
    "\n",
    "    return saved_paths\n",
    "\n",
    "\n",
    "for instances in INSTANCES_PER_BAG_GRID:\n",
    "    inst_cfg = config.for_instances(instances)\n",
    "    run_stage2_generation(inst_cfg, writer_indices, embed_dim)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}